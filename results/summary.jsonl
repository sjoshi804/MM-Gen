{"correct_answer_path": "/home/t-sijoshi/Phi3V-Finetuning/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/Phi3V-Finetuning/results/simple_prompt_visual_only_canonical_2024_07_17_18:15:18.jsonl", "num_correct": 860.0, "num_total": 1500, "accuracy": 0.5733333333333334, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/Phi3V-Finetuning/downloaded_datasets/spatial_map_canonical_500/text_only.json", "ai_answer_path": "/home/t-sijoshi/Phi3V-Finetuning/results/simple_prompt_text_only_canonical_2024_07_17_18:24:24.jsonl", "num_correct": 1043.0, "num_total": 1500, "accuracy": 0.6953333333333334, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/Phi3V-Finetuning/downloaded_datasets/spatial_map_canonical_500/visual_text.json", "ai_answer_path": "/home/t-sijoshi/Phi3V-Finetuning/results/simple_prompt_visual_text_canonical_2024_07_17_18:18:25.jsonl", "num_correct": 1097.0, "num_total": 1500, "accuracy": 0.7313333333333333, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_train/visual_only_1k.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/train_on_test_set_1k_2024_07_30_00:29:25.jsonl", "num_correct": 284.0, "num_total": 1000, "accuracy": 0.284, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_train/visual_only_1k.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/base_model_1k_2024_07_30_00:42:06.jsonl", "num_correct": 458.0, "num_total": 1000, "accuracy": 0.458, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_train/visual_only_1k.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/test_train_1k_ft_10_epoch_2024_07_30_16:20:45.jsonl", "num_correct": 313.0, "num_total": 1000, "accuracy": 0.313, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_train/visual_only_1k.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/test_train_1k_ft_10_epoch_2500_2024_07_30_16:28:49.jsonl", "num_correct": 313.0, "num_total": 1000, "accuracy": 0.313, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/spatial_map_500_canonical_vt_train_test_2024_07_30_17:28:25.jsonl", "num_correct": 1125.0, "num_total": 1500, "accuracy": 0.75, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/spatial_map_500_canonical_vt_train_v_test_2024_07_30_17:28:54.jsonl", "num_correct": 774.0, "num_total": 1500, "accuracy": 0.516, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/spatial_map_500_canonical_base_v_test_2024_07_30_17:30:49.jsonl", "num_correct": 808.0, "num_total": 1500, "accuracy": 0.5386666666666666, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/spatial_map_500_canonical_base_vt_test_2024_07_30_17:30:20.jsonl", "num_correct": 1059.0, "num_total": 1500, "accuracy": 0.706, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/spatial_map_combined_train_v_test_2024_07_30_18:17:41.jsonl", "num_correct": 784.0, "num_total": 1500, "accuracy": 0.5226666666666666, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/spatial_map_500_canonical_freeze_vision_lora_2024_07_30_19:13:48.jsonl", "num_correct": 807.0, "num_total": 1500, "accuracy": 0.538, "accuracy_split_1": 0.27, "accuracy_split_2": 1.0, "accuracy_split_3": 0.344, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/spatial_map_combined_train_v_test_2024_07_30_18:17:41.jsonl", "num_correct": 784.0, "num_total": 1500, "accuracy": 0.5226666666666666, "accuracy_split_1": 0.306, "accuracy_split_2": 0.95, "accuracy_split_3": 0.312, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/spatial_map_500_canonical_base_v_test_2024_07_30_17:30:49.jsonl", "num_correct": 808.0, "num_total": 1500, "accuracy": 0.5386666666666666, "accuracy_split_1": 0.266, "accuracy_split_2": 1.0, "accuracy_split_3": 0.35, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/spatial_map_canonical_visual_only_proj_only_2024_07_30_23:17:22.jsonl", "num_correct": 782.0, "num_total": 1500, "accuracy": 0.5213333333333333, "accuracy_split_1": 0.278, "accuracy_split_2": 1.0, "accuracy_split_3": 0.286, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/spatial_map_canonical_visual_only_vision_and_proj_2024_07_31_00:17:53.jsonl", "num_correct": 807.0, "num_total": 1500, "accuracy": 0.538, "accuracy_split_1": 0.268, "accuracy_split_2": 1.0, "accuracy_split_3": 0.346, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/spatial_map_canonical_visual_only_all_2024_07_31_15:33:42.jsonl", "num_correct": 830.0, "num_total": 1500, "accuracy": 0.5533333333333333, "accuracy_split_1": 0.372, "accuracy_split_2": 0.962, "accuracy_split_3": 0.326, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/spatial_map_canonical_visual_only_all_lora_2024_07_31_15:37:30.jsonl", "num_correct": 736.0, "num_total": 1500, "accuracy": 0.49066666666666664, "accuracy_split_1": 0.236, "accuracy_split_2": 0.946, "accuracy_split_3": 0.29, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/spatial_map_canonical_visual_only_lm_lora_2024_07_31_15:36:53.jsonl", "num_correct": 802.0, "num_total": 1500, "accuracy": 0.5346666666666666, "accuracy_split_1": 0.288, "accuracy_split_2": 1.0, "accuracy_split_3": 0.316, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/spatial_map_canonical_visual_only_lm_only_2024_07_31_15:50:43.jsonl", "num_correct": 818.0, "num_total": 1500, "accuracy": 0.5453333333333333, "accuracy_split_1": 0.344, "accuracy_split_2": 0.964, "accuracy_split_3": 0.328, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/spatial_map_canonical_visual_only_proj_only_2024_07_30_23:17:22.jsonl", "num_correct": 782.0, "num_total": 1500, "accuracy": 0.5213333333333333, "accuracy_split_1": 0.278, "accuracy_split_2": 1.0, "accuracy_split_3": 0.286, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/debug_2024_07_31_17:14:39.jsonl", "num_correct": 825.0, "num_total": 1500, "accuracy": 0.55, "accuracy_split_1": 0.372, "accuracy_split_2": 0.964, "accuracy_split_3": 0.314, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/debug_2024_07_31_17:44:22.jsonl", "num_correct": 790.0, "num_total": 1500, "accuracy": 0.5266666666666666, "accuracy_split_1": 0.33, "accuracy_split_2": 0.956, "accuracy_split_3": 0.294, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/debug_2024_07_31_23:42:07.jsonl", "num_correct": 804.0, "num_total": 1500, "accuracy": 0.536, "accuracy_split_1": 0.318, "accuracy_split_2": 0.996, "accuracy_split_3": 0.294, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/text_only_reformat.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/debug_text_only_2024_08_01_15:43:04.jsonl", "num_correct": 832.0, "num_total": 1500, "accuracy": 0.5546666666666666, "accuracy_split_1": 0.366, "accuracy_split_2": 0.982, "accuracy_split_3": 0.316, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/text_only_reformat.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/base_text_only_2024_08_01_16:05:25.jsonl", "num_correct": 782.0, "num_total": 1500, "accuracy": 0.5213333333333333, "accuracy_split_1": 0.582, "accuracy_split_2": 0.486, "accuracy_split_3": 0.496, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/debug_v_only_all_2024_08_01_21:01:35.jsonl", "num_correct": 1500.0, "num_total": 1500, "accuracy": 1.0, "accuracy_split_1": 1.0, "accuracy_split_2": 1.0, "accuracy_split_3": 1.0, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/spatial_map_canonical_visual_only_base_2024_08_01_21:41:59.jsonl", "num_correct": 697.0, "num_total": 1500, "accuracy": 0.4646666666666667, "accuracy_split_1": 0.25, "accuracy_split_2": 0.996, "accuracy_split_3": 0.148, "name": "base model (no ft)", "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/spatial_map_train_30k_all_2024_08_02_05:24:49.jsonl", "num_correct": 1310.0, "num_total": 1500, "accuracy": 0.8733333333333333, "accuracy_split_1": 0.668, "accuracy_split_2": 1.0, "accuracy_split_3": 0.952, "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/spatial_map_train_30k_proj_only_2024_08_02_14:46:29.jsonl", "num_correct": 501.0, "num_total": 1500, "accuracy": 0.334, "accuracy_split_1": 0.002, "accuracy_split_2": 1.0, "accuracy_split_3": 0.0, "name": "proj_only", "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/spatial_map_train_30k_vision_proj_2024_08_02_14:48:58.jsonl", "num_correct": 558.0, "num_total": 1500, "accuracy": 0.372, "accuracy_split_1": 0.038, "accuracy_split_2": 0.964, "accuracy_split_3": 0.114, "name": "vision_proj", "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/spatial_map_train_30k_lm_proj_2024_08_02_14:49:08.jsonl", "num_correct": 875.0, "num_total": 1500, "accuracy": 0.5833333333333334, "accuracy_split_1": 0.138, "accuracy_split_2": 0.666, "accuracy_split_3": 0.946, "name": "lm_proj", "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/spatial_map_train_30k_all_lora_2024_08_02_14:50:08.jsonl", "num_correct": 507.0, "num_total": 1500, "accuracy": 0.338, "accuracy_split_1": 0.02, "accuracy_split_2": 0.992, "accuracy_split_3": 0.002, "name": "all_lora", "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/spatial_map_train_30k_all_2024_08_02_05:24:49.jsonl", "num_correct": 1310.0, "num_total": 1500, "accuracy": 0.8733333333333333, "accuracy_split_1": 0.668, "accuracy_split_2": 1.0, "accuracy_split_3": 0.952, "name": "iid_30k_1epoch", "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/spatial_map_iid_30k_train_5_epoch_2024_08_09_18:16:16.jsonl", "num_correct": 1500.0, "num_total": 1500, "accuracy": 1.0, "accuracy_split_1": -1500.0, "name": "iid_30k_5epoch", "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/spatial_map_iid_30k_train_2_epoch_2024_08_09_18:59:52.jsonl", "num_correct": 1489.0, "num_total": 1500, "accuracy": 0.9926666666666667, "accuracy_split_1": 0.996, "accuracy_split_2": 0.988, "accuracy_split_3": 0.994, "name": "iid_30k_2epoch", "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/train_on_skill_desc_2024_08_12_15:21:42.jsonl", "num_correct": 549.0, "num_total": 1500, "accuracy": 0.366, "accuracy_split_1": 0.014, "accuracy_split_2": 1.0, "accuracy_split_3": 0.084, "name": "train directly on skill", "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/train_on_aug_text_aug_img_skill_2024_08_12_15:23:09.jsonl", "num_correct": 1221.0, "num_total": 1500, "accuracy": 0.814, "accuracy_split_1": 0.852, "accuracy_split_2": 0.712, "accuracy_split_3": 0.878, "name": "augment images, generate qa", "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/train_on_aug_text_skill_15k_2024_08_12_15:22:23.jsonl", "num_correct": 1312.0, "num_total": 1500, "accuracy": 0.8746666666666667, "accuracy_split_1": 0.88, "accuracy_split_2": 0.812, "accuracy_split_3": 0.932, "name": "generate qa", "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/spatial_map_iid_15k_seed_17_2024_08_12_15:20:35.jsonl", "num_correct": 1499.0, "num_total": 1500, "accuracy": 0.9993333333333333, "accuracy_split_1": 0.998, "accuracy_split_2": 1.0, "accuracy_split_3": 1.0, "name": "iid generation (seed=17, size=15k)", "task": "spatial_map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/train_on_v0_2024_08_12_15:23:53.jsonl", "num_correct": 1344.0, "num_total": 1500, "accuracy": 0.896, "accuracy_split_1": 0.866, "accuracy_split_2": 0.918, "accuracy_split_3": 0.904, "name": "v0", "task": "spatial_map"}
{"name": "base model", "task": "mmc", "correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/mmc-benchmark/data.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/mmc_benchmark_base_2024_08_15_17:04:25.jsonl", "accuracy": 0.7323612417685795, "num_correct": 1557.0, "num_total": 2126, "sub_tasks": [{"name": "analysis", "frac_of_data": 0.12041392285983067, "num_correct": 173.0, "num_total": 256, "accuracy": 0.67578125}, {"name": "details", "frac_of_data": 0.15522107243650046, "num_correct": 269.0, "num_total": 330, "accuracy": 0.8151515151515152}, {"name": "arxiv_single", "frac_of_data": 0.02634054562558796, "num_correct": 40.0, "num_total": 56, "accuracy": 0.7142857142857143}, {"name": "arxiv_two", "frac_of_data": 0.024459078080903106, "num_correct": 36.0, "num_total": 52, "accuracy": 0.6923076923076923}, {"name": "type_test", "frac_of_data": 0.16933207902163688, "num_correct": 291.0, "num_total": 360, "accuracy": 0.8083333333333333}, {"name": "topic_test", "frac_of_data": 0.2521166509877705, "num_correct": 453.0, "num_total": 536, "accuracy": 0.8451492537313433}, {"name": "datatable_test", "frac_of_data": 0.18814675446848542, "num_correct": 203.0, "num_total": 400, "accuracy": 0.5075}, {"name": "jsondata", "frac_of_data": 0.045155221072436504, "num_correct": 62.0, "num_total": 96, "accuracy": 0.6458333333333334}, {"name": "stockdata", "frac_of_data": 0.01881467544684854, "num_correct": 30.0, "num_total": 40, "accuracy": 0.75}]}
{"task": "mmmu", "name": "v0", "metric_results_path": "/home/t-sijoshi/LFM-Eval-Understand/logs/MMMU_PIPELINE/v0/eval_report/metric_results.jsonl", "accuracy": 0.39222222222222225, "num_correct": 353, "num_total": 900, "sub_tasks": [{"name": "Art and Design", "frac_of_data": 0.13333333333333333, "num_correct": 57, "num_total": 120, "accuracy": 0.475}, {"name": "Business", "frac_of_data": 0.16666666666666666, "num_correct": 52, "num_total": 150, "accuracy": 0.3466666666666667}, {"name": "Science", "frac_of_data": 0.16666666666666666, "num_correct": 50, "num_total": 150, "accuracy": 0.3333333333333333}, {"name": "Health and Medicine", "frac_of_data": 0.16666666666666666, "num_correct": 61, "num_total": 150, "accuracy": 0.4066666666666667}, {"name": "Humanities and Social Science", "frac_of_data": 0.13333333333333333, "num_correct": 65, "num_total": 120, "accuracy": 0.5416666666666666}, {"name": "Tech and Engineering", "frac_of_data": 0.23333333333333334, "num_correct": 68, "num_total": 210, "accuracy": 0.3238095238095238}]}
{"task": "mmmu", "name": "train directly on skill", "metric_results_path": "/home/t-sijoshi/LFM-Eval-Understand/logs/MMMU_PIPELINE/train_on_skill_desc/eval_report/metric_results.jsonl", "accuracy": 0.25555555555555554, "num_correct": 230, "num_total": 900, "sub_tasks": [{"name": "Art and Design", "frac_of_data": 0.13333333333333333, "num_correct": 35, "num_total": 120, "accuracy": 0.2916666666666667}, {"name": "Business", "frac_of_data": 0.16666666666666666, "num_correct": 33, "num_total": 150, "accuracy": 0.22}, {"name": "Science", "frac_of_data": 0.16666666666666666, "num_correct": 35, "num_total": 150, "accuracy": 0.23333333333333334}, {"name": "Health and Medicine", "frac_of_data": 0.16666666666666666, "num_correct": 43, "num_total": 150, "accuracy": 0.2866666666666667}, {"name": "Humanities and Social Science", "frac_of_data": 0.13333333333333333, "num_correct": 41, "num_total": 120, "accuracy": 0.3416666666666667}, {"name": "Tech and Engineering", "frac_of_data": 0.23333333333333334, "num_correct": 43, "num_total": 210, "accuracy": 0.20476190476190476}]}
{"task": "mmmu", "name": "iid", "metric_results_path": "/home/t-sijoshi/LFM-Eval-Understand/logs/MMMU_PIPELINE/iid_15k_seed_17/eval_report/metric_results.jsonl", "accuracy": 0.32222222222222224, "num_correct": 290, "num_total": 900, "sub_tasks": [{"name": "Art and Design", "frac_of_data": 0.13333333333333333, "num_correct": 51, "num_total": 120, "accuracy": 0.425}, {"name": "Business", "frac_of_data": 0.16666666666666666, "num_correct": 46, "num_total": 150, "accuracy": 0.30666666666666664}, {"name": "Science", "frac_of_data": 0.16666666666666666, "num_correct": 34, "num_total": 150, "accuracy": 0.22666666666666666}, {"name": "Health and Medicine", "frac_of_data": 0.16666666666666666, "num_correct": 46, "num_total": 150, "accuracy": 0.30666666666666664}, {"name": "Humanities and Social Science", "frac_of_data": 0.13333333333333333, "num_correct": 59, "num_total": 120, "accuracy": 0.49166666666666664}, {"name": "Tech and Engineering", "frac_of_data": 0.23333333333333334, "num_correct": 54, "num_total": 210, "accuracy": 0.2571428571428571}]}
{"task": "mmmu", "name": "gpt-4o", "metric_results_path": "/home/t-sijoshi/LFM-Eval-Understand/logs/MMMU_PIPELINE/gpt-4o/eval_report/metric_results.jsonl", "accuracy": 0.5855555555555556, "num_correct": 527, "num_total": 900, "sub_tasks": [{"name": "Art and Design", "frac_of_data": 0.13333333333333333, "num_correct": 87, "num_total": 120, "accuracy": 0.725}, {"name": "Business", "frac_of_data": 0.16666666666666666, "num_correct": 73, "num_total": 150, "accuracy": 0.4866666666666667}, {"name": "Science", "frac_of_data": 0.16666666666666666, "num_correct": 81, "num_total": 150, "accuracy": 0.54}, {"name": "Health and Medicine", "frac_of_data": 0.16666666666666666, "num_correct": 98, "num_total": 150, "accuracy": 0.6533333333333333}, {"name": "Humanities and Social Science", "frac_of_data": 0.13333333333333333, "num_correct": 92, "num_total": 120, "accuracy": 0.7666666666666667}, {"name": "Tech and Engineering", "frac_of_data": 0.23333333333333334, "num_correct": 96, "num_total": 210, "accuracy": 0.45714285714285713}]}
{"task": "mmmu", "name": "generate qa", "metric_results_path": "/home/t-sijoshi/LFM-Eval-Understand/logs/MMMU_PIPELINE/aug_txt/eval_report/metric_results.jsonl", "accuracy": 0.3111111111111111, "num_correct": 280, "num_total": 900, "sub_tasks": [{"name": "Art and Design", "frac_of_data": 0.13333333333333333, "num_correct": 42, "num_total": 120, "accuracy": 0.35}, {"name": "Business", "frac_of_data": 0.16666666666666666, "num_correct": 35, "num_total": 150, "accuracy": 0.23333333333333334}, {"name": "Science", "frac_of_data": 0.16666666666666666, "num_correct": 37, "num_total": 150, "accuracy": 0.24666666666666667}, {"name": "Health and Medicine", "frac_of_data": 0.16666666666666666, "num_correct": 59, "num_total": 150, "accuracy": 0.3933333333333333}, {"name": "Humanities and Social Science", "frac_of_data": 0.13333333333333333, "num_correct": 46, "num_total": 120, "accuracy": 0.38333333333333336}, {"name": "Tech and Engineering", "frac_of_data": 0.23333333333333334, "num_correct": 61, "num_total": 210, "accuracy": 0.2904761904761905}]}
{"task": "mmmu", "name": "aug img, generate qa", "metric_results_path": "/home/t-sijoshi/LFM-Eval-Understand/logs/MMMU_PIPELINE/aug_txt_aug_img/eval_report/metric_results.jsonl", "accuracy": 0.35555555555555557, "num_correct": 320, "num_total": 900, "sub_tasks": [{"name": "Art and Design", "frac_of_data": 0.13333333333333333, "num_correct": 52, "num_total": 120, "accuracy": 0.43333333333333335}, {"name": "Business", "frac_of_data": 0.16666666666666666, "num_correct": 44, "num_total": 150, "accuracy": 0.29333333333333333}, {"name": "Science", "frac_of_data": 0.16666666666666666, "num_correct": 39, "num_total": 150, "accuracy": 0.26}, {"name": "Health and Medicine", "frac_of_data": 0.16666666666666666, "num_correct": 56, "num_total": 150, "accuracy": 0.37333333333333335}, {"name": "Humanities and Social Science", "frac_of_data": 0.13333333333333333, "num_correct": 59, "num_total": 120, "accuracy": 0.49166666666666664}, {"name": "Tech and Engineering", "frac_of_data": 0.23333333333333334, "num_correct": 70, "num_total": 210, "accuracy": 0.3333333333333333}]}
{"task": "mmmu", "name": "base model", "metric_results_path": "/home/t-sijoshi/LFM-Eval-Understand/logs/MMMU_PIPELINE/base_model/eval_report/metric_results.jsonl", "accuracy": 0.35888888888888887, "num_correct": 323, "num_total": 900, "sub_tasks": [{"name": "Art and Design", "frac_of_data": 0.13333333333333333, "num_correct": 57, "num_total": 120, "accuracy": 0.475}, {"name": "Business", "frac_of_data": 0.16666666666666666, "num_correct": 41, "num_total": 150, "accuracy": 0.2733333333333333}, {"name": "Science", "frac_of_data": 0.16666666666666666, "num_correct": 45, "num_total": 150, "accuracy": 0.3}, {"name": "Health and Medicine", "frac_of_data": 0.16666666666666666, "num_correct": 54, "num_total": 150, "accuracy": 0.36}, {"name": "Humanities and Social Science", "frac_of_data": 0.13333333333333333, "num_correct": 56, "num_total": 120, "accuracy": 0.4666666666666667}, {"name": "Tech and Engineering", "frac_of_data": 0.23333333333333334, "num_correct": 70, "num_total": 210, "accuracy": 0.3333333333333333}]}
{"task": "mmc", "name": "gpt-4o", "correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/mmc-benchmark/data.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/mmc_benchmark_gpt4o_2024_08_15_19:12:06.jsonl", "accuracy": 0.8466603951081844, "num_correct": 1800.0, "num_total": 2126, "sub_tasks": [{"name": "analysis", "frac_of_data": 0.12041392285983067, "num_correct": 191.0, "num_total": 256, "accuracy": 0.74609375}, {"name": "details", "frac_of_data": 0.15522107243650046, "num_correct": 256.0, "num_total": 330, "accuracy": 0.7757575757575758}, {"name": "arxiv_single", "frac_of_data": 0.02634054562558796, "num_correct": 53.0, "num_total": 56, "accuracy": 0.9464285714285714}, {"name": "arxiv_two", "frac_of_data": 0.024459078080903106, "num_correct": 47.0, "num_total": 52, "accuracy": 0.9038461538461539}, {"name": "type_test", "frac_of_data": 0.16933207902163688, "num_correct": 299.0, "num_total": 360, "accuracy": 0.8305555555555556}, {"name": "topic_test", "frac_of_data": 0.2521166509877705, "num_correct": 502.0, "num_total": 536, "accuracy": 0.9365671641791045}, {"name": "datatable_test", "frac_of_data": 0.18814675446848542, "num_correct": 333.0, "num_total": 400, "accuracy": 0.8325}, {"name": "jsondata", "frac_of_data": 0.045155221072436504, "num_correct": 81.0, "num_total": 96, "accuracy": 0.84375}, {"name": "stockdata", "frac_of_data": 0.01881467544684854, "num_correct": 38.0, "num_total": 40, "accuracy": 0.95}]}
{"task": "mmmu", "name": "v1", "metric_results_path": "/home/t-sijoshi/LFM-Eval-Understand/logs/MMMU_PIPELINE/08-21-science-improve/v1/eval_report/metric_results.jsonl", "accuracy": 0.2388888888888889, "num_correct": 215, "num_total": 900, "sub_tasks": [{"name": "Art and Design", "frac_of_data": 0.13333333333333333, "num_correct": 27, "num_total": 120, "accuracy": 0.225}, {"name": "Business", "frac_of_data": 0.16666666666666666, "num_correct": 37, "num_total": 150, "accuracy": 0.24666666666666667}, {"name": "Science", "frac_of_data": 0.16666666666666666, "num_correct": 36, "num_total": 150, "accuracy": 0.24}, {"name": "Health and Medicine", "frac_of_data": 0.16666666666666666, "num_correct": 31, "num_total": 150, "accuracy": 0.20666666666666667}, {"name": "Humanities and Social Science", "frac_of_data": 0.13333333333333333, "num_correct": 31, "num_total": 120, "accuracy": 0.25833333333333336}, {"name": "Tech and Engineering", "frac_of_data": 0.23333333333333334, "num_correct": 53, "num_total": 210, "accuracy": 0.2523809523809524}]}
{"task": "mmmu", "name": "train_on_skill_desc", "metric_results_path": "/home/t-sijoshi/LFM-Eval-Understand/logs/MMMU_PIPELINE/08-21-science-improve/train_on_skill_desc/eval_report/metric_results.jsonl", "accuracy": 0.2611111111111111, "num_correct": 235, "num_total": 900, "sub_tasks": [{"name": "Art and Design", "frac_of_data": 0.13333333333333333, "num_correct": 27, "num_total": 120, "accuracy": 0.225}, {"name": "Business", "frac_of_data": 0.16666666666666666, "num_correct": 33, "num_total": 150, "accuracy": 0.22}, {"name": "Science", "frac_of_data": 0.16666666666666666, "num_correct": 36, "num_total": 150, "accuracy": 0.24}, {"name": "Health and Medicine", "frac_of_data": 0.16666666666666666, "num_correct": 42, "num_total": 150, "accuracy": 0.28}, {"name": "Humanities and Social Science", "frac_of_data": 0.13333333333333333, "num_correct": 48, "num_total": 120, "accuracy": 0.4}, {"name": "Tech and Engineering", "frac_of_data": 0.23333333333333334, "num_correct": 49, "num_total": 210, "accuracy": 0.23333333333333334}]}
{"task": "mmmu", "name": "rand_img_gen_qa", "metric_results_path": "/home/t-sijoshi/LFM-Eval-Understand/logs/MMMU_PIPELINE/08-21-science-improve/rand_img_gen_qa/eval_report/metric_results.jsonl", "accuracy": 0.2811111111111111, "num_correct": 253, "num_total": 900, "sub_tasks": [{"name": "Art and Design", "frac_of_data": 0.13333333333333333, "num_correct": 27, "num_total": 120, "accuracy": 0.225}, {"name": "Business", "frac_of_data": 0.16666666666666666, "num_correct": 37, "num_total": 150, "accuracy": 0.24666666666666667}, {"name": "Science", "frac_of_data": 0.16666666666666666, "num_correct": 45, "num_total": 150, "accuracy": 0.3}, {"name": "Health and Medicine", "frac_of_data": 0.16666666666666666, "num_correct": 44, "num_total": 150, "accuracy": 0.29333333333333333}, {"name": "Humanities and Social Science", "frac_of_data": 0.13333333333333333, "num_correct": 39, "num_total": 120, "accuracy": 0.325}, {"name": "Tech and Engineering", "frac_of_data": 0.23333333333333334, "num_correct": 61, "num_total": 210, "accuracy": 0.2904761904761905}]}
{"task": "mmmu", "name": "qa_format_1", "metric_results_path": "/home/t-sijoshi/LFM-Eval-Understand/logs/MMMU_PIPELINE/08-21-science-improve/qa_format_1/eval_report/metric_results.jsonl", "accuracy": 0.37, "num_correct": 333, "num_total": 900, "sub_tasks": [{"name": "Art and Design", "frac_of_data": 0.13333333333333333, "num_correct": 59, "num_total": 120, "accuracy": 0.49166666666666664}, {"name": "Business", "frac_of_data": 0.16666666666666666, "num_correct": 53, "num_total": 150, "accuracy": 0.35333333333333333}, {"name": "Science", "frac_of_data": 0.16666666666666666, "num_correct": 41, "num_total": 150, "accuracy": 0.2733333333333333}, {"name": "Health and Medicine", "frac_of_data": 0.16666666666666666, "num_correct": 62, "num_total": 150, "accuracy": 0.41333333333333333}, {"name": "Humanities and Social Science", "frac_of_data": 0.13333333333333333, "num_correct": 60, "num_total": 120, "accuracy": 0.5}, {"name": "Tech and Engineering", "frac_of_data": 0.23333333333333334, "num_correct": 58, "num_total": 210, "accuracy": 0.2761904761904762}]}
{"task": "mmmu", "name": "gen_qa_format_2", "metric_results_path": "/home/t-sijoshi/LFM-Eval-Understand/logs/MMMU_PIPELINE/08-21-science-improve/gen_qa_format_2/eval_report/metric_results.jsonl", "accuracy": 0.35333333333333333, "num_correct": 318, "num_total": 900, "sub_tasks": [{"name": "Art and Design", "frac_of_data": 0.13333333333333333, "num_correct": 63, "num_total": 120, "accuracy": 0.525}, {"name": "Business", "frac_of_data": 0.16666666666666666, "num_correct": 45, "num_total": 150, "accuracy": 0.3}, {"name": "Science", "frac_of_data": 0.16666666666666666, "num_correct": 41, "num_total": 150, "accuracy": 0.2733333333333333}, {"name": "Health and Medicine", "frac_of_data": 0.16666666666666666, "num_correct": 50, "num_total": 150, "accuracy": 0.3333333333333333}, {"name": "Humanities and Social Science", "frac_of_data": 0.13333333333333333, "num_correct": 57, "num_total": 120, "accuracy": 0.475}, {"name": "Tech and Engineering", "frac_of_data": 0.23333333333333334, "num_correct": 62, "num_total": 210, "accuracy": 0.29523809523809524}]}
{"task": "mmmu", "name": "v1_format", "metric_results_path": "/home/t-sijoshi/LFM-Eval-Understand/logs/MMMU_PIPELINE/08-21-science-improve/v1_format/eval_report/metric_results.jsonl", "accuracy": 0.25, "num_correct": 225, "num_total": 900, "sub_tasks": [{"name": "Art and Design", "frac_of_data": 0.13333333333333333, "num_correct": 35, "num_total": 120, "accuracy": 0.2916666666666667}, {"name": "Business", "frac_of_data": 0.16666666666666666, "num_correct": 44, "num_total": 150, "accuracy": 0.29333333333333333}, {"name": "Science", "frac_of_data": 0.16666666666666666, "num_correct": 34, "num_total": 150, "accuracy": 0.22666666666666666}, {"name": "Health and Medicine", "frac_of_data": 0.16666666666666666, "num_correct": 37, "num_total": 150, "accuracy": 0.24666666666666667}, {"name": "Humanities and Social Science", "frac_of_data": 0.13333333333333333, "num_correct": 23, "num_total": 120, "accuracy": 0.19166666666666668}, {"name": "Tech and Engineering", "frac_of_data": 0.23333333333333334, "num_correct": 52, "num_total": 210, "accuracy": 0.24761904761904763}]}
{"task": "mathvista_testmini", "correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/mathvista-testmini/data.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/inference/mathvista_base_model_2024_08_23_20:50:25.jsonl", "accuracy": 0.441, "num_correct": 441, "num_total": 1000, "sub_tasks": [{"name": "scientific reasoning", "frac_of_data": 0.082, "num_correct": 53, "num_total": 82, "accuracy": 0.6463414634146342}, {"name": "numeric commonsense arithmetic reasoning", "frac_of_data": 0.103, "num_correct": 25, "num_total": 103, "accuracy": 0.24271844660194175}, {"name": "geometry reasoning algebraic reasoning", "frac_of_data": 0.213, "num_correct": 91, "num_total": 213, "accuracy": 0.4272300469483568}, {"name": "geometry reasoning arithmetic reasoning", "frac_of_data": 0.014, "num_correct": 8, "num_total": 14, "accuracy": 0.5714285714285714}, {"name": "numeric commonsense", "frac_of_data": 0.034, "num_correct": 9, "num_total": 34, "accuracy": 0.2647058823529412}, {"name": "arithmetic reasoning", "frac_of_data": 0.145, "num_correct": 52, "num_total": 145, "accuracy": 0.3586206896551724}, {"name": "logical reasoning", "frac_of_data": 0.023, "num_correct": 2, "num_total": 23, "accuracy": 0.08695652173913043}, {"name": "stats reasoning", "frac_of_data": 0.191, "num_correct": 119, "num_total": 191, "accuracy": 0.6230366492146597}, {"name": "algebraic reasoning", "frac_of_data": 0.056, "num_correct": 28, "num_total": 56, "accuracy": 0.5}, {"name": "arithmetic reasoning stats reasoning", "frac_of_data": 0.074, "num_correct": 33, "num_total": 74, "accuracy": 0.44594594594594594}, {"name": "scientific reasoning stats reasoning", "frac_of_data": 0.03, "num_correct": 15, "num_total": 30, "accuracy": 0.5}, {"name": "logical reasoning arithmetic reasoning", "frac_of_data": 0.01, "num_correct": 0, "num_total": 10, "accuracy": 0.0}, {"name": "numeric commonsense geometry reasoning", "frac_of_data": 0.004, "num_correct": 3, "num_total": 4, "accuracy": 0.75}, {"name": "logical reasoning algebraic reasoning", "frac_of_data": 0.001, "num_correct": 0, "num_total": 1, "accuracy": 0.0}, {"name": "algebraic reasoning arithmetic reasoning scientific reasoning", "frac_of_data": 0.002, "num_correct": 0, "num_total": 2, "accuracy": 0.0}, {"name": "geometry reasoning algebraic reasoning scientific reasoning", "frac_of_data": 0.002, "num_correct": 0, "num_total": 2, "accuracy": 0.0}, {"name": "geometry reasoning algebraic reasoning arithmetic reasoning scientific reasoning", "frac_of_data": 0.001, "num_correct": 0, "num_total": 1, "accuracy": 0.0}, {"name": "algebraic reasoning stats reasoning", "frac_of_data": 0.003, "num_correct": 2, "num_total": 3, "accuracy": 0.6666666666666666}, {"name": "numeric commonsense geometry reasoning arithmetic reasoning", "frac_of_data": 0.001, "num_correct": 0, "num_total": 1, "accuracy": 0.0}, {"name": "numeric commonsense arithmetic reasoning stats reasoning", "frac_of_data": 0.002, "num_correct": 1, "num_total": 2, "accuracy": 0.5}, {"name": "algebraic reasoning scientific reasoning", "frac_of_data": 0.003, "num_correct": 0, "num_total": 3, "accuracy": 0.0}, {"name": "geometry reasoning scientific reasoning", "frac_of_data": 0.001, "num_correct": 0, "num_total": 1, "accuracy": 0.0}, {"name": "logical reasoning geometry reasoning", "frac_of_data": 0.002, "num_correct": 0, "num_total": 2, "accuracy": 0.0}, {"name": "geometry reasoning", "frac_of_data": 0.001, "num_correct": 0, "num_total": 1, "accuracy": 0.0}, {"name": "logical reasoning stats reasoning", "frac_of_data": 0.001, "num_correct": 0, "num_total": 1, "accuracy": 0.0}, {"name": "arithmetic reasoning scientific reasoning", "frac_of_data": 0.001, "num_correct": 0, "num_total": 1, "accuracy": 0.0}]}
{"name": "phi_0", "accuracy": 0.441, "task": "MathVista", "sub_tasks": [{"name": "algebraic reasoning", "accuracy": 0.536}, {"name": "algebraic reasoning stats reasoning", "accuracy": 0.667}, {"name": "arithmetic reasoning", "accuracy": 0.359}, {"name": "arithmetic reasoning stats reasoning", "accuracy": 0.446}, {"name": "geometry reasoning algebraic reasoning", "accuracy": 0.408}, {"name": "geometry reasoning arithmetic reasoning", "accuracy": 0.5}, {"name": "logical reasoning", "accuracy": 0.087}, {"name": "numeric commonsense", "accuracy": 0.265}, {"name": "numeric commonsense arithmetic reasoning", "accuracy": 0.243}, {"name": "numeric commonsense arithmetic reasoning stats reasoning", "accuracy": 0.5}, {"name": "numeric commonsense geometry reasoning", "accuracy": 0.75}, {"name": "scientific reasoning", "accuracy": 0.646}, {"name": "scientific reasoning stats reasoning", "accuracy": 0.533}, {"name": "stats reasoning", "accuracy": 0.634}]}
{"name": "phi_0", "accuracy": 0.441, "task": "MathVista", "sub_tasks": [{"name": "algebraic reasoning", "accuracy": 0.536}, {"name": "algebraic reasoning stats reasoning", "accuracy": 0.667}, {"name": "arithmetic reasoning", "accuracy": 0.359}, {"name": "arithmetic reasoning stats reasoning", "accuracy": 0.446}, {"name": "geometry reasoning algebraic reasoning", "accuracy": 0.408}, {"name": "geometry reasoning arithmetic reasoning", "accuracy": 0.5}, {"name": "logical reasoning", "accuracy": 0.087}, {"name": "numeric commonsense", "accuracy": 0.265}, {"name": "numeric commonsense arithmetic reasoning", "accuracy": 0.243}, {"name": "numeric commonsense arithmetic reasoning stats reasoning", "accuracy": 0.5}, {"name": "numeric commonsense geometry reasoning", "accuracy": 0.75}, {"name": "scientific reasoning", "accuracy": 0.646}, {"name": "scientific reasoning stats reasoning", "accuracy": 0.533}, {"name": "stats reasoning", "accuracy": 0.634}]}
{"name": "phi_0", "accuracy": 0.441, "task": "MathVista", "sub_tasks": [{"name": "algebraic reasoning", "accuracy": 0.536}, {"name": "algebraic reasoning stats reasoning", "accuracy": 0.667}, {"name": "arithmetic reasoning", "accuracy": 0.359}, {"name": "arithmetic reasoning stats reasoning", "accuracy": 0.446}, {"name": "geometry reasoning algebraic reasoning", "accuracy": 0.408}, {"name": "geometry reasoning arithmetic reasoning", "accuracy": 0.5}, {"name": "logical reasoning", "accuracy": 0.087}, {"name": "numeric commonsense", "accuracy": 0.265}, {"name": "numeric commonsense arithmetic reasoning", "accuracy": 0.243}, {"name": "numeric commonsense arithmetic reasoning stats reasoning", "accuracy": 0.5}, {"name": "numeric commonsense geometry reasoning", "accuracy": 0.75}, {"name": "scientific reasoning", "accuracy": 0.646}, {"name": "scientific reasoning stats reasoning", "accuracy": 0.533}, {"name": "stats reasoning", "accuracy": 0.634}]}
{"name": "phi_2", "accuracy": 0.337, "task": "MathVista", "sub_tasks": [{"name": "algebraic reasoning_function plot", "accuracy": 0.291}, {"name": "algebraic reasoning_scatter plot", "accuracy": 1.0}, {"name": "arithmetic reasoning stats reasoning_bar chart", "accuracy": 0.25}, {"name": "arithmetic reasoning stats reasoning_scatter plot", "accuracy": 1.0}, {"name": "arithmetic reasoning stats reasoning_table", "accuracy": 0.328}, {"name": "arithmetic reasoning_abstract scene", "accuracy": 0.318}, {"name": "arithmetic reasoning_synthetic scene", "accuracy": 0.341}, {"name": "geometry reasoning algebraic reasoning_function plot", "accuracy": 0.333}, {"name": "geometry reasoning algebraic reasoning_geometry diagram", "accuracy": 0.277}, {"name": "geometry reasoning arithmetic reasoning_abstract scene", "accuracy": 0.5}, {"name": "geometry reasoning arithmetic reasoning_function plot", "accuracy": 1.0}, {"name": "geometry reasoning arithmetic reasoning_geometry diagram", "accuracy": 0.4}, {"name": "geometry reasoning arithmetic reasoning_synthetic scene", "accuracy": 1.0}, {"name": "logical reasoning algebraic reasoning_puzzle test", "accuracy": 1.0}, {"name": "logical reasoning arithmetic reasoning_puzzle test", "accuracy": 0.2}, {"name": "logical reasoning_puzzle test", "accuracy": 0.217}, {"name": "numeric commonsense arithmetic reasoning stats reasoning_table", "accuracy": 1.0}, {"name": "numeric commonsense arithmetic reasoning_abstract scene", "accuracy": 0.5}, {"name": "numeric commonsense arithmetic reasoning_natural image", "accuracy": 0.277}, {"name": "numeric commonsense geometry reasoning arithmetic reasoning_natural image", "accuracy": 1.0}, {"name": "numeric commonsense geometry reasoning_abstract scene", "accuracy": 1.0}, {"name": "numeric commonsense geometry reasoning_natural image", "accuracy": 0.333}, {"name": "numeric commonsense_abstract scene", "accuracy": 0.152}, {"name": "scientific reasoning stats reasoning_bar chart", "accuracy": 0.5}, {"name": "scientific reasoning stats reasoning_line plot", "accuracy": 0.222}, {"name": "scientific reasoning stats reasoning_map chart", "accuracy": 0.625}, {"name": "scientific reasoning_medical image", "accuracy": 0.667}, {"name": "scientific reasoning_natural image", "accuracy": 1.0}, {"name": "scientific reasoning_scientific figure", "accuracy": 0.364}, {"name": "stats reasoning_bar chart", "accuracy": 0.587}, {"name": "stats reasoning_document image", "accuracy": 0.333}, {"name": "stats reasoning_line plot", "accuracy": 0.433}, {"name": "stats reasoning_pie chart", "accuracy": 0.417}, {"name": "stats reasoning_scatter plot", "accuracy": 0.394}]}
{"name": "phi_2", "accuracy": 0.337, "task": "MathVista", "sub_tasks": [{"name": "algebraic reasoning_function plot", "accuracy": 0.291}, {"name": "algebraic reasoning_scatter plot", "accuracy": 1.0}, {"name": "arithmetic reasoning stats reasoning_bar chart", "accuracy": 0.25}, {"name": "arithmetic reasoning stats reasoning_scatter plot", "accuracy": 1.0}, {"name": "arithmetic reasoning stats reasoning_table", "accuracy": 0.328}, {"name": "arithmetic reasoning_abstract scene", "accuracy": 0.318}, {"name": "arithmetic reasoning_synthetic scene", "accuracy": 0.341}, {"name": "geometry reasoning algebraic reasoning_function plot", "accuracy": 0.333}, {"name": "geometry reasoning algebraic reasoning_geometry diagram", "accuracy": 0.277}, {"name": "geometry reasoning arithmetic reasoning_abstract scene", "accuracy": 0.5}, {"name": "geometry reasoning arithmetic reasoning_function plot", "accuracy": 1.0}, {"name": "geometry reasoning arithmetic reasoning_geometry diagram", "accuracy": 0.4}, {"name": "geometry reasoning arithmetic reasoning_synthetic scene", "accuracy": 1.0}, {"name": "logical reasoning algebraic reasoning_puzzle test", "accuracy": 1.0}, {"name": "logical reasoning arithmetic reasoning_puzzle test", "accuracy": 0.2}, {"name": "logical reasoning_puzzle test", "accuracy": 0.217}, {"name": "numeric commonsense arithmetic reasoning stats reasoning_table", "accuracy": 1.0}, {"name": "numeric commonsense arithmetic reasoning_abstract scene", "accuracy": 0.5}, {"name": "numeric commonsense arithmetic reasoning_natural image", "accuracy": 0.277}, {"name": "numeric commonsense geometry reasoning arithmetic reasoning_natural image", "accuracy": 1.0}, {"name": "numeric commonsense geometry reasoning_abstract scene", "accuracy": 1.0}, {"name": "numeric commonsense geometry reasoning_natural image", "accuracy": 0.333}, {"name": "numeric commonsense_abstract scene", "accuracy": 0.152}, {"name": "scientific reasoning stats reasoning_bar chart", "accuracy": 0.5}, {"name": "scientific reasoning stats reasoning_line plot", "accuracy": 0.222}, {"name": "scientific reasoning stats reasoning_map chart", "accuracy": 0.625}, {"name": "scientific reasoning_medical image", "accuracy": 0.667}, {"name": "scientific reasoning_natural image", "accuracy": 1.0}, {"name": "scientific reasoning_scientific figure", "accuracy": 0.364}, {"name": "stats reasoning_bar chart", "accuracy": 0.587}, {"name": "stats reasoning_document image", "accuracy": 0.333}, {"name": "stats reasoning_line plot", "accuracy": 0.433}, {"name": "stats reasoning_pie chart", "accuracy": 0.417}, {"name": "stats reasoning_scatter plot", "accuracy": 0.394}]}
{"name": "phi_2", "accuracy": 0.337, "task": "MathVista", "sub_tasks": [{"name": "algebraic reasoning_function plot", "accuracy": 0.291}, {"name": "algebraic reasoning_scatter plot", "accuracy": 1.0}, {"name": "arithmetic reasoning stats reasoning_bar chart", "accuracy": 0.25}, {"name": "arithmetic reasoning stats reasoning_scatter plot", "accuracy": 1.0}, {"name": "arithmetic reasoning stats reasoning_table", "accuracy": 0.328}, {"name": "arithmetic reasoning_abstract scene", "accuracy": 0.318}, {"name": "arithmetic reasoning_synthetic scene", "accuracy": 0.341}, {"name": "geometry reasoning algebraic reasoning_function plot", "accuracy": 0.333}, {"name": "geometry reasoning algebraic reasoning_geometry diagram", "accuracy": 0.277}, {"name": "geometry reasoning arithmetic reasoning_abstract scene", "accuracy": 0.5}, {"name": "geometry reasoning arithmetic reasoning_function plot", "accuracy": 1.0}, {"name": "geometry reasoning arithmetic reasoning_geometry diagram", "accuracy": 0.4}, {"name": "geometry reasoning arithmetic reasoning_synthetic scene", "accuracy": 1.0}, {"name": "logical reasoning algebraic reasoning_puzzle test", "accuracy": 1.0}, {"name": "logical reasoning arithmetic reasoning_puzzle test", "accuracy": 0.2}, {"name": "logical reasoning_puzzle test", "accuracy": 0.217}, {"name": "numeric commonsense arithmetic reasoning stats reasoning_table", "accuracy": 1.0}, {"name": "numeric commonsense arithmetic reasoning_abstract scene", "accuracy": 0.5}, {"name": "numeric commonsense arithmetic reasoning_natural image", "accuracy": 0.277}, {"name": "numeric commonsense geometry reasoning arithmetic reasoning_natural image", "accuracy": 1.0}, {"name": "numeric commonsense geometry reasoning_abstract scene", "accuracy": 1.0}, {"name": "numeric commonsense geometry reasoning_natural image", "accuracy": 0.333}, {"name": "numeric commonsense_abstract scene", "accuracy": 0.152}, {"name": "scientific reasoning stats reasoning_bar chart", "accuracy": 0.5}, {"name": "scientific reasoning stats reasoning_line plot", "accuracy": 0.222}, {"name": "scientific reasoning stats reasoning_map chart", "accuracy": 0.625}, {"name": "scientific reasoning_medical image", "accuracy": 0.667}, {"name": "scientific reasoning_natural image", "accuracy": 1.0}, {"name": "scientific reasoning_scientific figure", "accuracy": 0.364}, {"name": "stats reasoning_bar chart", "accuracy": 0.587}, {"name": "stats reasoning_document image", "accuracy": 0.333}, {"name": "stats reasoning_line plot", "accuracy": 0.433}, {"name": "stats reasoning_pie chart", "accuracy": 0.417}, {"name": "stats reasoning_scatter plot", "accuracy": 0.394}]}
{"name": "phi_3", "accuracy": 0.319, "task": "MathVista", "sub_tasks": [{"name": "abstract scene", "accuracy": 0.197}, {"name": "bar chart", "accuracy": 0.555}, {"name": "document image", "accuracy": 0.417}, {"name": "function plot", "accuracy": 0.323}, {"name": "geometry diagram", "accuracy": 0.278}, {"name": "line plot", "accuracy": 0.282}, {"name": "map chart", "accuracy": 0.5}, {"name": "medical image", "accuracy": 0.333}, {"name": "natural image", "accuracy": 0.275}, {"name": "pie chart", "accuracy": 0.333}, {"name": "puzzle test", "accuracy": 0.111}, {"name": "scatter plot", "accuracy": 0.444}, {"name": "scientific figure", "accuracy": 0.239}, {"name": "synthetic scene", "accuracy": 0.29}, {"name": "table", "accuracy": 0.4}]}
{"name": "phi_3", "accuracy": 0.319, "task": "MathVista", "sub_tasks": [{"name": "abstract scene", "accuracy": 0.197}, {"name": "bar chart", "accuracy": 0.555}, {"name": "document image", "accuracy": 0.417}, {"name": "function plot", "accuracy": 0.323}, {"name": "geometry diagram", "accuracy": 0.278}, {"name": "line plot", "accuracy": 0.282}, {"name": "map chart", "accuracy": 0.5}, {"name": "medical image", "accuracy": 0.333}, {"name": "natural image", "accuracy": 0.275}, {"name": "pie chart", "accuracy": 0.333}, {"name": "puzzle test", "accuracy": 0.111}, {"name": "scatter plot", "accuracy": 0.444}, {"name": "scientific figure", "accuracy": 0.239}, {"name": "synthetic scene", "accuracy": 0.29}, {"name": "table", "accuracy": 0.4}]}
{"name": "phi_3", "accuracy": 0.319, "task": "MathVista", "sub_tasks": [{"name": "abstract scene", "accuracy": 0.197}, {"name": "bar chart", "accuracy": 0.555}, {"name": "document image", "accuracy": 0.417}, {"name": "function plot", "accuracy": 0.323}, {"name": "geometry diagram", "accuracy": 0.278}, {"name": "line plot", "accuracy": 0.282}, {"name": "map chart", "accuracy": 0.5}, {"name": "medical image", "accuracy": 0.333}, {"name": "natural image", "accuracy": 0.275}, {"name": "pie chart", "accuracy": 0.333}, {"name": "puzzle test", "accuracy": 0.111}, {"name": "scatter plot", "accuracy": 0.444}, {"name": "scientific figure", "accuracy": 0.239}, {"name": "synthetic scene", "accuracy": 0.29}, {"name": "table", "accuracy": 0.4}]}
{"name": "phi_4", "accuracy": 0.333, "task": "MathVista", "sub_tasks": [{"name": "algebraic reasoning_function plot", "accuracy": 0.327}, {"name": "arithmetic reasoning stats reasoning_bar chart", "accuracy": 0.333}, {"name": "arithmetic reasoning stats reasoning_scatter plot", "accuracy": 1.0}, {"name": "arithmetic reasoning stats reasoning_table", "accuracy": 0.377}, {"name": "arithmetic reasoning_abstract scene", "accuracy": 0.364}, {"name": "arithmetic reasoning_synthetic scene", "accuracy": 0.35}, {"name": "geometry reasoning algebraic reasoning_geometry diagram", "accuracy": 0.296}, {"name": "geometry reasoning arithmetic reasoning_geometry diagram", "accuracy": 0.3}, {"name": "logical reasoning arithmetic reasoning_puzzle test", "accuracy": 0.2}, {"name": "logical reasoning_puzzle test", "accuracy": 0.261}, {"name": "numeric commonsense arithmetic reasoning_abstract scene", "accuracy": 0.5}, {"name": "numeric commonsense arithmetic reasoning_natural image", "accuracy": 0.208}, {"name": "numeric commonsense_abstract scene", "accuracy": 0.182}, {"name": "scientific reasoning stats reasoning_line plot", "accuracy": 0.222}, {"name": "scientific reasoning stats reasoning_map chart", "accuracy": 0.5}, {"name": "scientific reasoning stats reasoning_table", "accuracy": 0.375}, {"name": "scientific reasoning_medical image", "accuracy": 0.667}, {"name": "scientific reasoning_natural image", "accuracy": 1.0}, {"name": "scientific reasoning_scientific figure", "accuracy": 0.403}, {"name": "stats reasoning_bar chart", "accuracy": 0.558}, {"name": "stats reasoning_document image", "accuracy": 0.25}, {"name": "stats reasoning_line plot", "accuracy": 0.533}, {"name": "stats reasoning_pie chart", "accuracy": 0.167}, {"name": "stats reasoning_scatter plot", "accuracy": 0.394}]}
{"name": "phi_4", "accuracy": 0.333, "task": "MathVista", "sub_tasks": [{"name": "algebraic reasoning_function plot", "accuracy": 0.327}, {"name": "arithmetic reasoning stats reasoning_bar chart", "accuracy": 0.333}, {"name": "arithmetic reasoning stats reasoning_scatter plot", "accuracy": 1.0}, {"name": "arithmetic reasoning stats reasoning_table", "accuracy": 0.377}, {"name": "arithmetic reasoning_abstract scene", "accuracy": 0.364}, {"name": "arithmetic reasoning_synthetic scene", "accuracy": 0.35}, {"name": "geometry reasoning algebraic reasoning_geometry diagram", "accuracy": 0.296}, {"name": "geometry reasoning arithmetic reasoning_geometry diagram", "accuracy": 0.3}, {"name": "logical reasoning arithmetic reasoning_puzzle test", "accuracy": 0.2}, {"name": "logical reasoning_puzzle test", "accuracy": 0.261}, {"name": "numeric commonsense arithmetic reasoning_abstract scene", "accuracy": 0.5}, {"name": "numeric commonsense arithmetic reasoning_natural image", "accuracy": 0.208}, {"name": "numeric commonsense_abstract scene", "accuracy": 0.182}, {"name": "scientific reasoning stats reasoning_line plot", "accuracy": 0.222}, {"name": "scientific reasoning stats reasoning_map chart", "accuracy": 0.5}, {"name": "scientific reasoning stats reasoning_table", "accuracy": 0.375}, {"name": "scientific reasoning_medical image", "accuracy": 0.667}, {"name": "scientific reasoning_natural image", "accuracy": 1.0}, {"name": "scientific reasoning_scientific figure", "accuracy": 0.403}, {"name": "stats reasoning_bar chart", "accuracy": 0.558}, {"name": "stats reasoning_document image", "accuracy": 0.25}, {"name": "stats reasoning_line plot", "accuracy": 0.533}, {"name": "stats reasoning_pie chart", "accuracy": 0.167}, {"name": "stats reasoning_scatter plot", "accuracy": 0.394}]}
{"name": "phi_4", "accuracy": 0.333, "task": "MathVista", "sub_tasks": [{"name": "algebraic reasoning_function plot", "accuracy": 0.327}, {"name": "arithmetic reasoning stats reasoning_bar chart", "accuracy": 0.333}, {"name": "arithmetic reasoning stats reasoning_scatter plot", "accuracy": 1.0}, {"name": "arithmetic reasoning stats reasoning_table", "accuracy": 0.377}, {"name": "arithmetic reasoning_abstract scene", "accuracy": 0.364}, {"name": "arithmetic reasoning_synthetic scene", "accuracy": 0.35}, {"name": "geometry reasoning algebraic reasoning_geometry diagram", "accuracy": 0.296}, {"name": "geometry reasoning arithmetic reasoning_geometry diagram", "accuracy": 0.3}, {"name": "logical reasoning arithmetic reasoning_puzzle test", "accuracy": 0.2}, {"name": "logical reasoning_puzzle test", "accuracy": 0.261}, {"name": "numeric commonsense arithmetic reasoning_abstract scene", "accuracy": 0.5}, {"name": "numeric commonsense arithmetic reasoning_natural image", "accuracy": 0.208}, {"name": "numeric commonsense_abstract scene", "accuracy": 0.182}, {"name": "scientific reasoning stats reasoning_line plot", "accuracy": 0.222}, {"name": "scientific reasoning stats reasoning_map chart", "accuracy": 0.5}, {"name": "scientific reasoning stats reasoning_table", "accuracy": 0.375}, {"name": "scientific reasoning_medical image", "accuracy": 0.667}, {"name": "scientific reasoning_natural image", "accuracy": 1.0}, {"name": "scientific reasoning_scientific figure", "accuracy": 0.403}, {"name": "stats reasoning_bar chart", "accuracy": 0.558}, {"name": "stats reasoning_document image", "accuracy": 0.25}, {"name": "stats reasoning_line plot", "accuracy": 0.533}, {"name": "stats reasoning_pie chart", "accuracy": 0.167}, {"name": "stats reasoning_scatter plot", "accuracy": 0.394}]}
{"name": "phi_6", "accuracy": 0.39, "task": "MathVista", "sub_tasks": [{"name": "algebraic reasoning", "accuracy": 0.446}, {"name": "algebraic reasoning stats reasoning", "accuracy": 1.0}, {"name": "arithmetic reasoning", "accuracy": 0.303}, {"name": "arithmetic reasoning stats reasoning", "accuracy": 0.514}, {"name": "geometry reasoning algebraic reasoning", "accuracy": 0.244}, {"name": "geometry reasoning arithmetic reasoning", "accuracy": 0.5}, {"name": "logical reasoning", "accuracy": 0.13}, {"name": "logical reasoning stats reasoning", "accuracy": 1.0}, {"name": "numeric commonsense", "accuracy": 0.353}, {"name": "numeric commonsense arithmetic reasoning", "accuracy": 0.233}, {"name": "numeric commonsense arithmetic reasoning stats reasoning", "accuracy": 0.5}, {"name": "numeric commonsense geometry reasoning", "accuracy": 0.5}, {"name": "numeric commonsense geometry reasoning arithmetic reasoning", "accuracy": 1.0}, {"name": "scientific reasoning", "accuracy": 0.622}, {"name": "scientific reasoning stats reasoning", "accuracy": 0.467}, {"name": "stats reasoning", "accuracy": 0.586}]}
{"name": "phi_6", "accuracy": 0.39, "task": "MathVista", "sub_tasks": [{"name": "algebraic reasoning", "accuracy": 0.446}, {"name": "algebraic reasoning stats reasoning", "accuracy": 1.0}, {"name": "arithmetic reasoning", "accuracy": 0.303}, {"name": "arithmetic reasoning stats reasoning", "accuracy": 0.514}, {"name": "geometry reasoning algebraic reasoning", "accuracy": 0.244}, {"name": "geometry reasoning arithmetic reasoning", "accuracy": 0.5}, {"name": "logical reasoning", "accuracy": 0.13}, {"name": "logical reasoning stats reasoning", "accuracy": 1.0}, {"name": "numeric commonsense", "accuracy": 0.353}, {"name": "numeric commonsense arithmetic reasoning", "accuracy": 0.233}, {"name": "numeric commonsense arithmetic reasoning stats reasoning", "accuracy": 0.5}, {"name": "numeric commonsense geometry reasoning", "accuracy": 0.5}, {"name": "numeric commonsense geometry reasoning arithmetic reasoning", "accuracy": 1.0}, {"name": "scientific reasoning", "accuracy": 0.622}, {"name": "scientific reasoning stats reasoning", "accuracy": 0.467}, {"name": "stats reasoning", "accuracy": 0.586}]}
{"name": "phi_6", "accuracy": 0.39, "task": "MathVista", "sub_tasks": [{"name": "algebraic reasoning", "accuracy": 0.446}, {"name": "algebraic reasoning stats reasoning", "accuracy": 1.0}, {"name": "arithmetic reasoning", "accuracy": 0.303}, {"name": "arithmetic reasoning stats reasoning", "accuracy": 0.514}, {"name": "geometry reasoning algebraic reasoning", "accuracy": 0.244}, {"name": "geometry reasoning arithmetic reasoning", "accuracy": 0.5}, {"name": "logical reasoning", "accuracy": 0.13}, {"name": "logical reasoning stats reasoning", "accuracy": 1.0}, {"name": "numeric commonsense", "accuracy": 0.353}, {"name": "numeric commonsense arithmetic reasoning", "accuracy": 0.233}, {"name": "numeric commonsense arithmetic reasoning stats reasoning", "accuracy": 0.5}, {"name": "numeric commonsense geometry reasoning", "accuracy": 0.5}, {"name": "numeric commonsense geometry reasoning arithmetic reasoning", "accuracy": 1.0}, {"name": "scientific reasoning", "accuracy": 0.622}, {"name": "scientific reasoning stats reasoning", "accuracy": 0.467}, {"name": "stats reasoning", "accuracy": 0.586}]}
{"name": "phi_7", "accuracy": 0.4, "task": "MathVista", "sub_tasks": [{"name": "algebraic reasoning stats reasoning_function plot", "accuracy": 1.0}, {"name": "algebraic reasoning_function plot", "accuracy": 0.455}, {"name": "algebraic reasoning_scatter plot", "accuracy": 1.0}, {"name": "arithmetic reasoning stats reasoning_bar chart", "accuracy": 0.5}, {"name": "arithmetic reasoning stats reasoning_scatter plot", "accuracy": 1.0}, {"name": "arithmetic reasoning stats reasoning_table", "accuracy": 0.525}, {"name": "arithmetic reasoning_abstract scene", "accuracy": 0.455}, {"name": "arithmetic reasoning_synthetic scene", "accuracy": 0.382}, {"name": "geometry reasoning algebraic reasoning_geometry diagram", "accuracy": 0.262}, {"name": "geometry reasoning arithmetic reasoning_geometry diagram", "accuracy": 0.7}, {"name": "geometry reasoning arithmetic reasoning_synthetic scene", "accuracy": 1.0}, {"name": "logical reasoning arithmetic reasoning_puzzle test", "accuracy": 0.1}, {"name": "logical reasoning_puzzle test", "accuracy": 0.13}, {"name": "numeric commonsense arithmetic reasoning stats reasoning_table", "accuracy": 1.0}, {"name": "numeric commonsense arithmetic reasoning_natural image", "accuracy": 0.267}, {"name": "numeric commonsense geometry reasoning_natural image", "accuracy": 0.667}, {"name": "numeric commonsense_abstract scene", "accuracy": 0.485}, {"name": "scientific reasoning stats reasoning_bar chart", "accuracy": 0.5}, {"name": "scientific reasoning stats reasoning_line plot", "accuracy": 0.778}, {"name": "scientific reasoning stats reasoning_map chart", "accuracy": 0.375}, {"name": "scientific reasoning stats reasoning_table", "accuracy": 0.25}, {"name": "scientific reasoning stats reasoning_violin plot", "accuracy": 1.0}, {"name": "scientific reasoning_medical image", "accuracy": 0.667}, {"name": "scientific reasoning_natural image", "accuracy": 1.0}, {"name": "scientific reasoning_scientific figure", "accuracy": 0.636}, {"name": "stats reasoning_bar chart", "accuracy": 0.548}, {"name": "stats reasoning_document image", "accuracy": 0.417}, {"name": "stats reasoning_line plot", "accuracy": 0.333}, {"name": "stats reasoning_pie chart", "accuracy": 0.667}, {"name": "stats reasoning_scatter plot", "accuracy": 0.485}]}
{"name": "phi_7", "accuracy": 0.4, "task": "MathVista", "sub_tasks": [{"name": "algebraic reasoning stats reasoning_function plot", "accuracy": 1.0}, {"name": "algebraic reasoning_function plot", "accuracy": 0.455}, {"name": "algebraic reasoning_scatter plot", "accuracy": 1.0}, {"name": "arithmetic reasoning stats reasoning_bar chart", "accuracy": 0.5}, {"name": "arithmetic reasoning stats reasoning_scatter plot", "accuracy": 1.0}, {"name": "arithmetic reasoning stats reasoning_table", "accuracy": 0.525}, {"name": "arithmetic reasoning_abstract scene", "accuracy": 0.455}, {"name": "arithmetic reasoning_synthetic scene", "accuracy": 0.382}, {"name": "geometry reasoning algebraic reasoning_geometry diagram", "accuracy": 0.262}, {"name": "geometry reasoning arithmetic reasoning_geometry diagram", "accuracy": 0.7}, {"name": "geometry reasoning arithmetic reasoning_synthetic scene", "accuracy": 1.0}, {"name": "logical reasoning arithmetic reasoning_puzzle test", "accuracy": 0.1}, {"name": "logical reasoning_puzzle test", "accuracy": 0.13}, {"name": "numeric commonsense arithmetic reasoning stats reasoning_table", "accuracy": 1.0}, {"name": "numeric commonsense arithmetic reasoning_natural image", "accuracy": 0.267}, {"name": "numeric commonsense geometry reasoning_natural image", "accuracy": 0.667}, {"name": "numeric commonsense_abstract scene", "accuracy": 0.485}, {"name": "scientific reasoning stats reasoning_bar chart", "accuracy": 0.5}, {"name": "scientific reasoning stats reasoning_line plot", "accuracy": 0.778}, {"name": "scientific reasoning stats reasoning_map chart", "accuracy": 0.375}, {"name": "scientific reasoning stats reasoning_table", "accuracy": 0.25}, {"name": "scientific reasoning stats reasoning_violin plot", "accuracy": 1.0}, {"name": "scientific reasoning_medical image", "accuracy": 0.667}, {"name": "scientific reasoning_natural image", "accuracy": 1.0}, {"name": "scientific reasoning_scientific figure", "accuracy": 0.636}, {"name": "stats reasoning_bar chart", "accuracy": 0.548}, {"name": "stats reasoning_document image", "accuracy": 0.417}, {"name": "stats reasoning_line plot", "accuracy": 0.333}, {"name": "stats reasoning_pie chart", "accuracy": 0.667}, {"name": "stats reasoning_scatter plot", "accuracy": 0.485}]}
{"name": "phi_7", "accuracy": 0.4, "task": "MathVista", "sub_tasks": [{"name": "algebraic reasoning stats reasoning_function plot", "accuracy": 1.0}, {"name": "algebraic reasoning_function plot", "accuracy": 0.455}, {"name": "algebraic reasoning_scatter plot", "accuracy": 1.0}, {"name": "arithmetic reasoning stats reasoning_bar chart", "accuracy": 0.5}, {"name": "arithmetic reasoning stats reasoning_scatter plot", "accuracy": 1.0}, {"name": "arithmetic reasoning stats reasoning_table", "accuracy": 0.525}, {"name": "arithmetic reasoning_abstract scene", "accuracy": 0.455}, {"name": "arithmetic reasoning_synthetic scene", "accuracy": 0.382}, {"name": "geometry reasoning algebraic reasoning_geometry diagram", "accuracy": 0.262}, {"name": "geometry reasoning arithmetic reasoning_geometry diagram", "accuracy": 0.7}, {"name": "geometry reasoning arithmetic reasoning_synthetic scene", "accuracy": 1.0}, {"name": "logical reasoning arithmetic reasoning_puzzle test", "accuracy": 0.1}, {"name": "logical reasoning_puzzle test", "accuracy": 0.13}, {"name": "numeric commonsense arithmetic reasoning stats reasoning_table", "accuracy": 1.0}, {"name": "numeric commonsense arithmetic reasoning_natural image", "accuracy": 0.267}, {"name": "numeric commonsense geometry reasoning_natural image", "accuracy": 0.667}, {"name": "numeric commonsense_abstract scene", "accuracy": 0.485}, {"name": "scientific reasoning stats reasoning_bar chart", "accuracy": 0.5}, {"name": "scientific reasoning stats reasoning_line plot", "accuracy": 0.778}, {"name": "scientific reasoning stats reasoning_map chart", "accuracy": 0.375}, {"name": "scientific reasoning stats reasoning_table", "accuracy": 0.25}, {"name": "scientific reasoning stats reasoning_violin plot", "accuracy": 1.0}, {"name": "scientific reasoning_medical image", "accuracy": 0.667}, {"name": "scientific reasoning_natural image", "accuracy": 1.0}, {"name": "scientific reasoning_scientific figure", "accuracy": 0.636}, {"name": "stats reasoning_bar chart", "accuracy": 0.548}, {"name": "stats reasoning_document image", "accuracy": 0.417}, {"name": "stats reasoning_line plot", "accuracy": 0.333}, {"name": "stats reasoning_pie chart", "accuracy": 0.667}, {"name": "stats reasoning_scatter plot", "accuracy": 0.485}]}
{"name": "phi_8", "accuracy": 0.409, "task": "MathVista", "sub_tasks": [{"name": "algebraic reasoning", "accuracy": 0.446}, {"name": "algebraic reasoning stats reasoning", "accuracy": 0.333}, {"name": "arithmetic reasoning", "accuracy": 0.331}, {"name": "arithmetic reasoning stats reasoning", "accuracy": 0.595}, {"name": "geometry reasoning algebraic reasoning", "accuracy": 0.268}, {"name": "geometry reasoning arithmetic reasoning", "accuracy": 0.643}, {"name": "logical reasoning", "accuracy": 0.217}, {"name": "logical reasoning algebraic reasoning", "accuracy": 1.0}, {"name": "logical reasoning arithmetic reasoning", "accuracy": 0.1}, {"name": "numeric commonsense", "accuracy": 0.294}, {"name": "numeric commonsense arithmetic reasoning", "accuracy": 0.252}, {"name": "numeric commonsense arithmetic reasoning stats reasoning", "accuracy": 0.5}, {"name": "numeric commonsense geometry reasoning", "accuracy": 0.75}, {"name": "scientific reasoning", "accuracy": 0.585}, {"name": "scientific reasoning stats reasoning", "accuracy": 0.567}, {"name": "stats reasoning", "accuracy": 0.592}]}
{"name": "phi_8", "accuracy": 0.409, "task": "MathVista", "sub_tasks": [{"name": "algebraic reasoning", "accuracy": 0.446}, {"name": "algebraic reasoning stats reasoning", "accuracy": 0.333}, {"name": "arithmetic reasoning", "accuracy": 0.331}, {"name": "arithmetic reasoning stats reasoning", "accuracy": 0.595}, {"name": "geometry reasoning algebraic reasoning", "accuracy": 0.268}, {"name": "geometry reasoning arithmetic reasoning", "accuracy": 0.643}, {"name": "logical reasoning", "accuracy": 0.217}, {"name": "logical reasoning algebraic reasoning", "accuracy": 1.0}, {"name": "logical reasoning arithmetic reasoning", "accuracy": 0.1}, {"name": "numeric commonsense", "accuracy": 0.294}, {"name": "numeric commonsense arithmetic reasoning", "accuracy": 0.252}, {"name": "numeric commonsense arithmetic reasoning stats reasoning", "accuracy": 0.5}, {"name": "numeric commonsense geometry reasoning", "accuracy": 0.75}, {"name": "scientific reasoning", "accuracy": 0.585}, {"name": "scientific reasoning stats reasoning", "accuracy": 0.567}, {"name": "stats reasoning", "accuracy": 0.592}]}
{"name": "phi_8", "accuracy": 0.409, "task": "MathVista", "sub_tasks": [{"name": "algebraic reasoning", "accuracy": 0.446}, {"name": "algebraic reasoning stats reasoning", "accuracy": 0.333}, {"name": "arithmetic reasoning", "accuracy": 0.331}, {"name": "arithmetic reasoning stats reasoning", "accuracy": 0.595}, {"name": "geometry reasoning algebraic reasoning", "accuracy": 0.268}, {"name": "geometry reasoning arithmetic reasoning", "accuracy": 0.643}, {"name": "logical reasoning", "accuracy": 0.217}, {"name": "logical reasoning algebraic reasoning", "accuracy": 1.0}, {"name": "logical reasoning arithmetic reasoning", "accuracy": 0.1}, {"name": "numeric commonsense", "accuracy": 0.294}, {"name": "numeric commonsense arithmetic reasoning", "accuracy": 0.252}, {"name": "numeric commonsense arithmetic reasoning stats reasoning", "accuracy": 0.5}, {"name": "numeric commonsense geometry reasoning", "accuracy": 0.75}, {"name": "scientific reasoning", "accuracy": 0.585}, {"name": "scientific reasoning stats reasoning", "accuracy": 0.567}, {"name": "stats reasoning", "accuracy": 0.592}]}
{"name": "phi_9", "accuracy": 0.332, "task": "MathVista", "sub_tasks": [{"name": "algebraic reasoning", "accuracy": 0.357}, {"name": "arithmetic reasoning", "accuracy": 0.283}, {"name": "arithmetic reasoning stats reasoning", "accuracy": 0.419}, {"name": "geometry reasoning algebraic reasoning", "accuracy": 0.286}, {"name": "geometry reasoning arithmetic reasoning", "accuracy": 0.286}, {"name": "logical reasoning", "accuracy": 0.087}, {"name": "logical reasoning arithmetic reasoning", "accuracy": 0.2}, {"name": "numeric commonsense", "accuracy": 0.147}, {"name": "numeric commonsense arithmetic reasoning", "accuracy": 0.272}, {"name": "numeric commonsense arithmetic reasoning stats reasoning", "accuracy": 0.5}, {"name": "numeric commonsense geometry reasoning", "accuracy": 0.5}, {"name": "numeric commonsense geometry reasoning arithmetic reasoning", "accuracy": 1.0}, {"name": "scientific reasoning", "accuracy": 0.305}, {"name": "scientific reasoning stats reasoning", "accuracy": 0.3}, {"name": "stats reasoning", "accuracy": 0.524}]}
{"name": "phi_9", "accuracy": 0.332, "task": "MathVista", "sub_tasks": [{"name": "algebraic reasoning", "accuracy": 0.357}, {"name": "arithmetic reasoning", "accuracy": 0.283}, {"name": "arithmetic reasoning stats reasoning", "accuracy": 0.419}, {"name": "geometry reasoning algebraic reasoning", "accuracy": 0.286}, {"name": "geometry reasoning arithmetic reasoning", "accuracy": 0.286}, {"name": "logical reasoning", "accuracy": 0.087}, {"name": "logical reasoning arithmetic reasoning", "accuracy": 0.2}, {"name": "numeric commonsense", "accuracy": 0.147}, {"name": "numeric commonsense arithmetic reasoning", "accuracy": 0.272}, {"name": "numeric commonsense arithmetic reasoning stats reasoning", "accuracy": 0.5}, {"name": "numeric commonsense geometry reasoning", "accuracy": 0.5}, {"name": "numeric commonsense geometry reasoning arithmetic reasoning", "accuracy": 1.0}, {"name": "scientific reasoning", "accuracy": 0.305}, {"name": "scientific reasoning stats reasoning", "accuracy": 0.3}, {"name": "stats reasoning", "accuracy": 0.524}]}
{"name": "phi_9", "accuracy": 0.332, "task": "MathVista", "sub_tasks": [{"name": "algebraic reasoning", "accuracy": 0.357}, {"name": "arithmetic reasoning", "accuracy": 0.283}, {"name": "arithmetic reasoning stats reasoning", "accuracy": 0.419}, {"name": "geometry reasoning algebraic reasoning", "accuracy": 0.286}, {"name": "geometry reasoning arithmetic reasoning", "accuracy": 0.286}, {"name": "logical reasoning", "accuracy": 0.087}, {"name": "logical reasoning arithmetic reasoning", "accuracy": 0.2}, {"name": "numeric commonsense", "accuracy": 0.147}, {"name": "numeric commonsense arithmetic reasoning", "accuracy": 0.272}, {"name": "numeric commonsense arithmetic reasoning stats reasoning", "accuracy": 0.5}, {"name": "numeric commonsense geometry reasoning", "accuracy": 0.5}, {"name": "numeric commonsense geometry reasoning arithmetic reasoning", "accuracy": 1.0}, {"name": "scientific reasoning", "accuracy": 0.305}, {"name": "scientific reasoning stats reasoning", "accuracy": 0.3}, {"name": "stats reasoning", "accuracy": 0.524}]}
{"name": "phi_10", "accuracy": 0.426, "task": "MathVista", "sub_tasks": [{"name": "abstract scene", "accuracy": 0.377}, {"name": "bar chart", "accuracy": 0.714}, {"name": "document image", "accuracy": 0.417}, {"name": "function plot", "accuracy": 0.5}, {"name": "geometry diagram", "accuracy": 0.352}, {"name": "line plot", "accuracy": 0.513}, {"name": "map chart", "accuracy": 0.5}, {"name": "medical image", "accuracy": 0.333}, {"name": "natural image", "accuracy": 0.257}, {"name": "pie chart", "accuracy": 0.583}, {"name": "puzzle test", "accuracy": 0.25}, {"name": "scatter plot", "accuracy": 0.556}, {"name": "scientific figure", "accuracy": 0.511}, {"name": "synthetic scene", "accuracy": 0.282}, {"name": "table", "accuracy": 0.486}, {"name": "violin plot", "accuracy": 1.0}]}
{"name": "phi_10", "accuracy": 0.426, "task": "MathVista", "sub_tasks": [{"name": "abstract scene", "accuracy": 0.377}, {"name": "bar chart", "accuracy": 0.714}, {"name": "document image", "accuracy": 0.417}, {"name": "function plot", "accuracy": 0.5}, {"name": "geometry diagram", "accuracy": 0.352}, {"name": "line plot", "accuracy": 0.513}, {"name": "map chart", "accuracy": 0.5}, {"name": "medical image", "accuracy": 0.333}, {"name": "natural image", "accuracy": 0.257}, {"name": "pie chart", "accuracy": 0.583}, {"name": "puzzle test", "accuracy": 0.25}, {"name": "scatter plot", "accuracy": 0.556}, {"name": "scientific figure", "accuracy": 0.511}, {"name": "synthetic scene", "accuracy": 0.282}, {"name": "table", "accuracy": 0.486}, {"name": "violin plot", "accuracy": 1.0}]}
{"name": "phi_10", "accuracy": 0.426, "task": "MathVista", "sub_tasks": [{"name": "abstract scene", "accuracy": 0.377}, {"name": "bar chart", "accuracy": 0.714}, {"name": "document image", "accuracy": 0.417}, {"name": "function plot", "accuracy": 0.5}, {"name": "geometry diagram", "accuracy": 0.352}, {"name": "line plot", "accuracy": 0.513}, {"name": "map chart", "accuracy": 0.5}, {"name": "medical image", "accuracy": 0.333}, {"name": "natural image", "accuracy": 0.257}, {"name": "pie chart", "accuracy": 0.583}, {"name": "puzzle test", "accuracy": 0.25}, {"name": "scatter plot", "accuracy": 0.556}, {"name": "scientific figure", "accuracy": 0.511}, {"name": "synthetic scene", "accuracy": 0.282}, {"name": "table", "accuracy": 0.486}, {"name": "violin plot", "accuracy": 1.0}]}
{"name": "llava_0", "accuracy": 0.237, "task": "MathVista", "sub_tasks": [{"name": "algebraic reasoning statistical reasoning_function plot", "accuracy": 0.333}, {"name": "algebraic reasoning_function plot", "accuracy": 0.4}, {"name": "algebraic reasoning_scatter plot", "accuracy": 1.0}, {"name": "arithmetic reasoning statistical reasoning_bar chart", "accuracy": 0.083}, {"name": "arithmetic reasoning statistical reasoning_table", "accuracy": 0.164}, {"name": "arithmetic reasoning_abstract scene", "accuracy": 0.091}, {"name": "arithmetic reasoning_synthetic scene", "accuracy": 0.301}, {"name": "geometry reasoning algebraic reasoning_geometry diagram", "accuracy": 0.184}, {"name": "geometry reasoning arithmetic reasoning_abstract scene", "accuracy": 0.5}, {"name": "geometry reasoning arithmetic reasoning_geometry diagram", "accuracy": 0.1}, {"name": "logical reasoning arithmetic reasoning_puzzle test", "accuracy": 0.2}, {"name": "logical reasoning_puzzle test", "accuracy": 0.174}, {"name": "numeric commonsense arithmetic reasoning statistical reasoning_natural image", "accuracy": 1.0}, {"name": "numeric commonsense arithmetic reasoning_natural image", "accuracy": 0.168}, {"name": "numeric commonsense geometry reasoning arithmetic reasoning_natural image", "accuracy": 1.0}, {"name": "numeric commonsense geometry reasoning_natural image", "accuracy": 0.667}, {"name": "numeric commonsense_natural image", "accuracy": 1.0}, {"name": "scientific reasoning statistical reasoning_line plot", "accuracy": 0.222}, {"name": "scientific reasoning statistical reasoning_map chart", "accuracy": 0.375}, {"name": "scientific reasoning_medical image", "accuracy": 1.0}, {"name": "scientific reasoning_natural image", "accuracy": 0.5}, {"name": "scientific reasoning_scientific figure", "accuracy": 0.442}, {"name": "statistical reasoning_bar chart", "accuracy": 0.192}, {"name": "statistical reasoning_document image", "accuracy": 0.167}, {"name": "statistical reasoning_line plot", "accuracy": 0.333}, {"name": "statistical reasoning_pie chart", "accuracy": 0.5}, {"name": "statistical reasoning_scatter plot", "accuracy": 0.424}]}
{"name": "llava_0", "accuracy": 0.237, "task": "MathVista", "sub_tasks": [{"name": "algebraic reasoning statistical reasoning_function plot", "accuracy": 0.333}, {"name": "algebraic reasoning_function plot", "accuracy": 0.4}, {"name": "algebraic reasoning_scatter plot", "accuracy": 1.0}, {"name": "arithmetic reasoning statistical reasoning_bar chart", "accuracy": 0.083}, {"name": "arithmetic reasoning statistical reasoning_table", "accuracy": 0.164}, {"name": "arithmetic reasoning_abstract scene", "accuracy": 0.091}, {"name": "arithmetic reasoning_synthetic scene", "accuracy": 0.301}, {"name": "geometry reasoning algebraic reasoning_geometry diagram", "accuracy": 0.184}, {"name": "geometry reasoning arithmetic reasoning_abstract scene", "accuracy": 0.5}, {"name": "geometry reasoning arithmetic reasoning_geometry diagram", "accuracy": 0.1}, {"name": "logical reasoning arithmetic reasoning_puzzle test", "accuracy": 0.2}, {"name": "logical reasoning_puzzle test", "accuracy": 0.174}, {"name": "numeric commonsense arithmetic reasoning statistical reasoning_natural image", "accuracy": 1.0}, {"name": "numeric commonsense arithmetic reasoning_natural image", "accuracy": 0.168}, {"name": "numeric commonsense geometry reasoning arithmetic reasoning_natural image", "accuracy": 1.0}, {"name": "numeric commonsense geometry reasoning_natural image", "accuracy": 0.667}, {"name": "numeric commonsense_natural image", "accuracy": 1.0}, {"name": "scientific reasoning statistical reasoning_line plot", "accuracy": 0.222}, {"name": "scientific reasoning statistical reasoning_map chart", "accuracy": 0.375}, {"name": "scientific reasoning_medical image", "accuracy": 1.0}, {"name": "scientific reasoning_natural image", "accuracy": 0.5}, {"name": "scientific reasoning_scientific figure", "accuracy": 0.442}, {"name": "statistical reasoning_bar chart", "accuracy": 0.192}, {"name": "statistical reasoning_document image", "accuracy": 0.167}, {"name": "statistical reasoning_line plot", "accuracy": 0.333}, {"name": "statistical reasoning_pie chart", "accuracy": 0.5}, {"name": "statistical reasoning_scatter plot", "accuracy": 0.424}]}
{"name": "llava_0", "accuracy": 0.237, "task": "MathVista", "sub_tasks": [{"name": "algebraic reasoning statistical reasoning_function plot", "accuracy": 0.333}, {"name": "algebraic reasoning_function plot", "accuracy": 0.4}, {"name": "algebraic reasoning_scatter plot", "accuracy": 1.0}, {"name": "arithmetic reasoning statistical reasoning_bar chart", "accuracy": 0.083}, {"name": "arithmetic reasoning statistical reasoning_table", "accuracy": 0.164}, {"name": "arithmetic reasoning_abstract scene", "accuracy": 0.091}, {"name": "arithmetic reasoning_synthetic scene", "accuracy": 0.301}, {"name": "geometry reasoning algebraic reasoning_geometry diagram", "accuracy": 0.184}, {"name": "geometry reasoning arithmetic reasoning_abstract scene", "accuracy": 0.5}, {"name": "geometry reasoning arithmetic reasoning_geometry diagram", "accuracy": 0.1}, {"name": "logical reasoning arithmetic reasoning_puzzle test", "accuracy": 0.2}, {"name": "logical reasoning_puzzle test", "accuracy": 0.174}, {"name": "numeric commonsense arithmetic reasoning statistical reasoning_natural image", "accuracy": 1.0}, {"name": "numeric commonsense arithmetic reasoning_natural image", "accuracy": 0.168}, {"name": "numeric commonsense geometry reasoning arithmetic reasoning_natural image", "accuracy": 1.0}, {"name": "numeric commonsense geometry reasoning_natural image", "accuracy": 0.667}, {"name": "numeric commonsense_natural image", "accuracy": 1.0}, {"name": "scientific reasoning statistical reasoning_line plot", "accuracy": 0.222}, {"name": "scientific reasoning statistical reasoning_map chart", "accuracy": 0.375}, {"name": "scientific reasoning_medical image", "accuracy": 1.0}, {"name": "scientific reasoning_natural image", "accuracy": 0.5}, {"name": "scientific reasoning_scientific figure", "accuracy": 0.442}, {"name": "statistical reasoning_bar chart", "accuracy": 0.192}, {"name": "statistical reasoning_document image", "accuracy": 0.167}, {"name": "statistical reasoning_line plot", "accuracy": 0.333}, {"name": "statistical reasoning_pie chart", "accuracy": 0.5}, {"name": "statistical reasoning_scatter plot", "accuracy": 0.424}]}
{"name": "llava_8", "accuracy": 0.271, "task": "MathVista", "sub_tasks": [{"name": "abstract scene", "accuracy": 0.311}, {"name": "bar chart", "accuracy": 0.235}, {"name": "document image", "accuracy": 0.167}, {"name": "function plot", "accuracy": 0.258}, {"name": "geometry diagram", "accuracy": 0.255}, {"name": "line plot", "accuracy": 0.333}, {"name": "map chart", "accuracy": 0.375}, {"name": "medical image", "accuracy": 1.0}, {"name": "natural image", "accuracy": 0.202}, {"name": "pie chart", "accuracy": 0.333}, {"name": "puzzle test", "accuracy": 0.222}, {"name": "scatter plot", "accuracy": 0.361}, {"name": "scientific figure", "accuracy": 0.293}, {"name": "synthetic scene", "accuracy": 0.363}, {"name": "table", "accuracy": 0.186}]}
{"name": "llava_1", "accuracy": 0.272, "task": "MathVista", "sub_tasks": [{"name": "algebraic reasoning", "accuracy": 0.411}, {"name": "arithmetic reasoning", "accuracy": 0.297}, {"name": "arithmetic reasoning statistical reasoning", "accuracy": 0.176}, {"name": "geometry reasoning algebraic reasoning", "accuracy": 0.244}, {"name": "geometry reasoning arithmetic reasoning", "accuracy": 0.643}, {"name": "logical reasoning", "accuracy": 0.217}, {"name": "numeric commonsense", "accuracy": 0.382}, {"name": "numeric commonsense arithmetic reasoning", "accuracy": 0.136}, {"name": "numeric commonsense arithmetic reasoning statistical reasoning", "accuracy": 0.5}, {"name": "numeric commonsense geometry reasoning", "accuracy": 0.75}, {"name": "numeric commonsense geometry reasoning arithmetic reasoning", "accuracy": 1.0}, {"name": "scientific reasoning", "accuracy": 0.476}, {"name": "scientific reasoning statistical reasoning", "accuracy": 0.233}, {"name": "statistical reasoning", "accuracy": 0.257}]}
{"name": "llava_7", "accuracy": 0.333, "task": "MathVista", "sub_tasks": [{"name": "algebraic reasoning statistical reasoning_function plot", "accuracy": 0.667}, {"name": "algebraic reasoning_function plot", "accuracy": 0.4}, {"name": "algebraic reasoning_scatter plot", "accuracy": 1.0}, {"name": "arithmetic reasoning statistical reasoning_bar chart", "accuracy": 0.167}, {"name": "arithmetic reasoning statistical reasoning_table", "accuracy": 0.262}, {"name": "arithmetic reasoning_abstract scene", "accuracy": 0.318}, {"name": "arithmetic reasoning_synthetic scene", "accuracy": 0.407}, {"name": "geometry reasoning algebraic reasoning_geometry diagram", "accuracy": 0.316}, {"name": "geometry reasoning arithmetic reasoning_abstract scene", "accuracy": 0.5}, {"name": "geometry reasoning arithmetic reasoning_geometry diagram", "accuracy": 0.9}, {"name": "logical reasoning arithmetic reasoning_puzzle test", "accuracy": 0.2}, {"name": "logical reasoning_puzzle test", "accuracy": 0.217}, {"name": "numeric commonsense arithmetic reasoning_abstract scene", "accuracy": 0.5}, {"name": "numeric commonsense arithmetic reasoning_natural image", "accuracy": 0.218}, {"name": "numeric commonsense geometry reasoning_abstract scene", "accuracy": 1.0}, {"name": "numeric commonsense geometry reasoning_natural image", "accuracy": 0.667}, {"name": "numeric commonsense_abstract scene", "accuracy": 0.394}, {"name": "scientific reasoning statistical reasoning_bar chart", "accuracy": 0.5}, {"name": "scientific reasoning statistical reasoning_line plot", "accuracy": 0.333}, {"name": "scientific reasoning statistical reasoning_map chart", "accuracy": 0.375}, {"name": "scientific reasoning_medical image", "accuracy": 0.333}, {"name": "scientific reasoning_natural image", "accuracy": 1.0}, {"name": "scientific reasoning_scientific figure", "accuracy": 0.481}, {"name": "statistical reasoning_bar chart", "accuracy": 0.317}, {"name": "statistical reasoning_document image", "accuracy": 0.25}, {"name": "statistical reasoning_line plot", "accuracy": 0.4}, {"name": "statistical reasoning_pie chart", "accuracy": 0.417}, {"name": "statistical reasoning_scatter plot", "accuracy": 0.364}]}
{"name": "llava_cqa_trainset_skyline", "accuracy": 0.4744, "task": "ChartQA", "sub_tasks": [{"name": "relaxed_augmented_split", "accuracy": 0.5864}, {"name": "relaxed_human_split", "accuracy": 0.3624}]}
{"name": "llava_cqa_v1_old", "accuracy": 0.2848, "task": "ChartQA", "sub_tasks": [{"name": "relaxed_augmented_split", "accuracy": 0.2664}, {"name": "relaxed_human_split", "accuracy": 0.3032}]}
{"name": "llava_base_model", "accuracy": 0.1824, "task": "ChartQA", "sub_tasks": [{"name": "relaxed_augmented_split", "accuracy": 0.2224}, {"name": "relaxed_human_split", "accuracy": 0.1424}]}
{"name": "llava_cqa_v1_train_longer", "accuracy": 0.3268, "task": "ChartQA", "sub_tasks": [{"name": "relaxed_augmented_split", "accuracy": 0.3432}, {"name": "relaxed_human_split", "accuracy": 0.3104}]}
{"name": "llava_cqa_v1", "accuracy": 0.2992, "task": "ChartQA", "sub_tasks": [{"name": "relaxed_augmented_split", "accuracy": 0.2904}, {"name": "relaxed_human_split", "accuracy": 0.3080}]}
{"name": "llava_cqa_v1_wo_skill_desc", "accuracy": 0.288, "task": "ChartQA", "sub_tasks": [{"name": "relaxed_augmented_split", "accuracy": 0.3096}, {"name": "relaxed_human_split", "accuracy": 0.2664}]}
{"name": "llava_base_model", "accuracy": 0.55214, "task": "ai2d"}
{"name": "llava_skyline_(training_data)", "accuracy": 0.63601, "task": "ai2d"}
{"name": "llava_us_(50_training_examples_error_desc)", "accuracy": 0.64637 , "task": "ai2d"}
{"name": "llava_augmenting_skyline", "accuracy": 0.66354, "task": "ai2d"}
{"name": "Base Model", "accuracy": 0.55214, "task": "AI2D"}
{"name": "MMInstruct (100K)", "accuracy": 0.68329, "task": "AI2D"}
{"name": "Skyline (Provided Training Data)", "accuracy": 0.63601, "task": "AI2D"}
{"name": "Base Model", "accuracy": 0.1824, "task": "ChartQA", "sub_tasks": [{"name": "relaxed_augmented_split", "accuracy": 0.2224}, {"name": "relaxed_human_split", "accuracy": 0.1424}]}
{"name": "Generic Caption (150K)", "accuracy": 0.2376, "task": "ChartQA"}
{"name": "MMInstruct (150K)", "accuracy": 0.3288, "task": "ChartQA"}
{"name": "Skyline (Provided Training Data)", "accuracy": 0.4744, "task": "ChartQA", "sub_tasks": [{"name": "relaxed_augmented_split", "accuracy": 0.5864}, {"name": "relaxed_human_split", "accuracy": 0.3624}]}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/spatial_map_iid_15k_seed_17_2024_08_12_15:20:35.jsonl", "num_correct": 1499.0, "num_total": 1500, "accuracy": 0.9993333333333333, "accuracy_split_1": 0.998, "accuracy_split_2": 1.0, "accuracy_split_3": 1.0, "name": "Skyline (Provided Training Data)", "task": "Spatial Map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/train_on_v0_2024_08_12_15:23:53.jsonl", "num_correct": 1344.0, "num_total": 1500, "accuracy": 0.896, "accuracy_split_1": 0.866, "accuracy_split_2": 0.918, "accuracy_split_3": 0.904, "name": "MMInstruct", "task": "Spatial Map"}
{"correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/spatial_map_canonical_500/visual_only.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/spatial_map_canonical_visual_only_base_2024_08_01_21:41:59.jsonl", "num_correct": 697.0, "num_total": 1500, "accuracy": 0.4646666666666667, "accuracy_split_1": 0.25, "accuracy_split_2": 0.996, "accuracy_split_3": 0.148, "name": "Base Model", "task": "Spatial Map"}
{"task": "MMMU", "name": "MMInstruct", "metric_results_path": "/home/t-sijoshi/LFM-Eval-Understand/logs/MMMU_PIPELINE/v0/eval_report/metric_results.jsonl", "accuracy": 0.39222222222222225, "num_correct": 353, "num_total": 900, "sub_tasks": [{"name": "Art and Design", "frac_of_data": 0.13333333333333333, "num_correct": 57, "num_total": 120, "accuracy": 0.475}, {"name": "Business", "frac_of_data": 0.16666666666666666, "num_correct": 52, "num_total": 150, "accuracy": 0.3466666666666667}, {"name": "Science", "frac_of_data": 0.16666666666666666, "num_correct": 50, "num_total": 150, "accuracy": 0.3333333333333333}, {"name": "Health and Medicine", "frac_of_data": 0.16666666666666666, "num_correct": 61, "num_total": 150, "accuracy": 0.4066666666666667}, {"name": "Humanities and Social Science", "frac_of_data": 0.13333333333333333, "num_correct": 65, "num_total": 120, "accuracy": 0.5416666666666666}, {"name": "Tech and Engineering", "frac_of_data": 0.23333333333333334, "num_correct": 68, "num_total": 210, "accuracy": 0.3238095238095238}]}
{"task": "mmmu", "name": "train directly on skill", "metric_results_path": "/home/t-sijoshi/LFM-Eval-Understand/logs/MMMU_PIPELINE/train_on_skill_desc/eval_report/metric_results.jsonl", "accuracy": 0.25555555555555554, "num_correct": 230, "num_total": 900, "sub_tasks": [{"name": "Art and Design", "frac_of_data": 0.13333333333333333, "num_correct": 35, "num_total": 120, "accuracy": 0.2916666666666667}, {"name": "Business", "frac_of_data": 0.16666666666666666, "num_correct": 33, "num_total": 150, "accuracy": 0.22}, {"name": "Science", "frac_of_data": 0.16666666666666666, "num_correct": 35, "num_total": 150, "accuracy": 0.23333333333333334}, {"name": "Health and Medicine", "frac_of_data": 0.16666666666666666, "num_correct": 43, "num_total": 150, "accuracy": 0.2866666666666667}, {"name": "Humanities and Social Science", "frac_of_data": 0.13333333333333333, "num_correct": 41, "num_total": 120, "accuracy": 0.3416666666666667}, {"name": "Tech and Engineering", "frac_of_data": 0.23333333333333334, "num_correct": 43, "num_total": 210, "accuracy": 0.20476190476190476}]}
{"task": "MMMU", "name": "Skyline (Provided Training Data)", "metric_results_path": "/home/t-sijoshi/LFM-Eval-Understand/logs/MMMU_PIPELINE/iid_15k_seed_17/eval_report/metric_results.jsonl", "accuracy": 0.32222222222222224, "num_correct": 290, "num_total": 900, "sub_tasks": [{"name": "Art and Design", "frac_of_data": 0.13333333333333333, "num_correct": 51, "num_total": 120, "accuracy": 0.425}, {"name": "Business", "frac_of_data": 0.16666666666666666, "num_correct": 46, "num_total": 150, "accuracy": 0.30666666666666664}, {"name": "Science", "frac_of_data": 0.16666666666666666, "num_correct": 34, "num_total": 150, "accuracy": 0.22666666666666666}, {"name": "Health and Medicine", "frac_of_data": 0.16666666666666666, "num_correct": 46, "num_total": 150, "accuracy": 0.30666666666666664}, {"name": "Humanities and Social Science", "frac_of_data": 0.13333333333333333, "num_correct": 59, "num_total": 120, "accuracy": 0.49166666666666664}, {"name": "Tech and Engineering", "frac_of_data": 0.23333333333333334, "num_correct": 54, "num_total": 210, "accuracy": 0.2571428571428571}]}
{"task": "mmmu", "name": "gpt-4o", "metric_results_path": "/home/t-sijoshi/LFM-Eval-Understand/logs/MMMU_PIPELINE/gpt-4o/eval_report/metric_results.jsonl", "accuracy": 0.5855555555555556, "num_correct": 527, "num_total": 900, "sub_tasks": [{"name": "Art and Design", "frac_of_data": 0.13333333333333333, "num_correct": 87, "num_total": 120, "accuracy": 0.725}, {"name": "Business", "frac_of_data": 0.16666666666666666, "num_correct": 73, "num_total": 150, "accuracy": 0.4866666666666667}, {"name": "Science", "frac_of_data": 0.16666666666666666, "num_correct": 81, "num_total": 150, "accuracy": 0.54}, {"name": "Health and Medicine", "frac_of_data": 0.16666666666666666, "num_correct": 98, "num_total": 150, "accuracy": 0.6533333333333333}, {"name": "Humanities and Social Science", "frac_of_data": 0.13333333333333333, "num_correct": 92, "num_total": 120, "accuracy": 0.7666666666666667}, {"name": "Tech and Engineering", "frac_of_data": 0.23333333333333334, "num_correct": 96, "num_total": 210, "accuracy": 0.45714285714285713}]}
{"task": "mmmu", "name": "generate qa", "metric_results_path": "/home/t-sijoshi/LFM-Eval-Understand/logs/MMMU_PIPELINE/aug_txt/eval_report/metric_results.jsonl", "accuracy": 0.3111111111111111, "num_correct": 280, "num_total": 900, "sub_tasks": [{"name": "Art and Design", "frac_of_data": 0.13333333333333333, "num_correct": 42, "num_total": 120, "accuracy": 0.35}, {"name": "Business", "frac_of_data": 0.16666666666666666, "num_correct": 35, "num_total": 150, "accuracy": 0.23333333333333334}, {"name": "Science", "frac_of_data": 0.16666666666666666, "num_correct": 37, "num_total": 150, "accuracy": 0.24666666666666667}, {"name": "Health and Medicine", "frac_of_data": 0.16666666666666666, "num_correct": 59, "num_total": 150, "accuracy": 0.3933333333333333}, {"name": "Humanities and Social Science", "frac_of_data": 0.13333333333333333, "num_correct": 46, "num_total": 120, "accuracy": 0.38333333333333336}, {"name": "Tech and Engineering", "frac_of_data": 0.23333333333333334, "num_correct": 61, "num_total": 210, "accuracy": 0.2904761904761905}]}
{"task": "mmmu", "name": "aug img, generate qa", "metric_results_path": "/home/t-sijoshi/LFM-Eval-Understand/logs/MMMU_PIPELINE/aug_txt_aug_img/eval_report/metric_results.jsonl", "accuracy": 0.35555555555555557, "num_correct": 320, "num_total": 900, "sub_tasks": [{"name": "Art and Design", "frac_of_data": 0.13333333333333333, "num_correct": 52, "num_total": 120, "accuracy": 0.43333333333333335}, {"name": "Business", "frac_of_data": 0.16666666666666666, "num_correct": 44, "num_total": 150, "accuracy": 0.29333333333333333}, {"name": "Science", "frac_of_data": 0.16666666666666666, "num_correct": 39, "num_total": 150, "accuracy": 0.26}, {"name": "Health and Medicine", "frac_of_data": 0.16666666666666666, "num_correct": 56, "num_total": 150, "accuracy": 0.37333333333333335}, {"name": "Humanities and Social Science", "frac_of_data": 0.13333333333333333, "num_correct": 59, "num_total": 120, "accuracy": 0.49166666666666664}, {"name": "Tech and Engineering", "frac_of_data": 0.23333333333333334, "num_correct": 70, "num_total": 210, "accuracy": 0.3333333333333333}]}
{"task": "MMMU", "name": "Base Model", "metric_results_path": "/home/t-sijoshi/LFM-Eval-Understand/logs/MMMU_PIPELINE/base_model/eval_report/metric_results.jsonl", "accuracy": 0.35888888888888887, "num_correct": 323, "num_total": 900, "sub_tasks": [{"name": "Art and Design", "frac_of_data": 0.13333333333333333, "num_correct": 57, "num_total": 120, "accuracy": 0.475}, {"name": "Business", "frac_of_data": 0.16666666666666666, "num_correct": 41, "num_total": 150, "accuracy": 0.2733333333333333}, {"name": "Science", "frac_of_data": 0.16666666666666666, "num_correct": 45, "num_total": 150, "accuracy": 0.3}, {"name": "Health and Medicine", "frac_of_data": 0.16666666666666666, "num_correct": 54, "num_total": 150, "accuracy": 0.36}, {"name": "Humanities and Social Science", "frac_of_data": 0.13333333333333333, "num_correct": 56, "num_total": 120, "accuracy": 0.4666666666666667}, {"name": "Tech and Engineering", "frac_of_data": 0.23333333333333334, "num_correct": 70, "num_total": 210, "accuracy": 0.3333333333333333}]}
{"task": "mmc", "name": "gpt-4o", "correct_answer_path": "/home/t-sijoshi/multimodal-data-gen/downloaded_datasets/mmc-benchmark/data.json", "ai_answer_path": "/home/t-sijoshi/multimodal-data-gen/results/mmc_benchmark_gpt4o_2024_08_15_19:12:06.jsonl", "accuracy": 0.8466603951081844, "num_correct": 1800.0, "num_total": 2126, "sub_tasks": [{"name": "analysis", "frac_of_data": 0.12041392285983067, "num_correct": 191.0, "num_total": 256, "accuracy": 0.74609375}, {"name": "details", "frac_of_data": 0.15522107243650046, "num_correct": 256.0, "num_total": 330, "accuracy": 0.7757575757575758}, {"name": "arxiv_single", "frac_of_data": 0.02634054562558796, "num_correct": 53.0, "num_total": 56, "accuracy": 0.9464285714285714}, {"name": "arxiv_two", "frac_of_data": 0.024459078080903106, "num_correct": 47.0, "num_total": 52, "accuracy": 0.9038461538461539}, {"name": "type_test", "frac_of_data": 0.16933207902163688, "num_correct": 299.0, "num_total": 360, "accuracy": 0.8305555555555556}, {"name": "topic_test", "frac_of_data": 0.2521166509877705, "num_correct": 502.0, "num_total": 536, "accuracy": 0.9365671641791045}, {"name": "datatable_test", "frac_of_data": 0.18814675446848542, "num_correct": 333.0, "num_total": 400, "accuracy": 0.8325}, {"name": "jsondata", "frac_of_data": 0.045155221072436504, "num_correct": 81.0, "num_total": 96, "accuracy": 0.84375}, {"name": "stockdata", "frac_of_data": 0.01881467544684854, "num_correct": 38.0, "num_total": 40, "accuracy": 0.95}]}
{"task": "ChartQA", "accuracy": 0.30, "name": "Filtered 100K from 250K MMInstruct v0 (0.6x training time)"}
{"task": "ChartQA", "accuracy": 0.30, "name": "250K MMInstruct v0 (1x training time)"}
{"name": "Base Model (LLava 1.5)", "accuracy": 0.329, "task": "MMMU", "sub_tasks": [{"name": "Art and Design", "accuracy": 0.45}, {"name": "Business", "accuracy": 0.24}, {"name": "Health and Medicine", "accuracy": 0.313}, {"name": "Humanities and Social Science", "accuracy": 0.458}, {"name": "Science", "accuracy": 0.26}, {"name": "Tech and Engineering", "accuracy": 0.31}]}
{"name": "AI2D MMInstruct (100K)", "accuracy": 0.221, "task": "MMMU", "sub_tasks": [{"name": "Art and Design", "accuracy": 0.225}, {"name": "Business", "accuracy": 0.167}, {"name": "Health and Medicine", "accuracy": 0.26}, {"name": "Humanities and Social Science", "accuracy": 0.225}, {"name": "Science", "accuracy": 0.207}, {"name": "Tech and Engineering", "accuracy": 0.238}]}
{"name": "AI2D Skyline (Training Data)", "accuracy": 0.23, "task": "MMMU", "sub_tasks": [{"name": "Art and Design", "accuracy": 0.275}, {"name": "Business", "accuracy": 0.187}, {"name": "Health and Medicine", "accuracy": 0.307}, {"name": "Humanities and Social Science", "accuracy": 0.167}, {"name": "Science", "accuracy": 0.213}, {"name": "Tech and Engineering", "accuracy": 0.229}]}
{"name": "ChartQA MMInstruct (150K)", "accuracy": 0.327, "task": "MMMU", "sub_tasks": [{"name": "Art and Design", "accuracy": 0.342}, {"name": "Business", "accuracy": 0.287}, {"name": "Health and Medicine", "accuracy": 0.373}, {"name": "Humanities and Social Science", "accuracy": 0.45}, {"name": "Science", "accuracy": 0.247}, {"name": "Tech and Engineering", "accuracy": 0.3}]}
{"name": "ChartQA Skyline (Training Data)", "accuracy": 0.342, "task": "MMMU", "sub_tasks": [{"name": "Art and Design", "accuracy": 0.442}, {"name": "Business", "accuracy": 0.273}, {"name": "Health and Medicine", "accuracy": 0.333}, {"name": "Humanities and Social Science", "accuracy": 0.475}, {"name": "Science", "accuracy": 0.253}, {"name": "Tech and Engineering", "accuracy": 0.329}]}
{"task": "ChartQA", "accuracy": 0.3064, "name": "MMInstruct 75K from 150K Subset (Random Subset)", "sub_tasks": [{"name": "relaxed_augmented_split", "accuracy": 0.30}, {"name": "relaxed_human_split", "accuracy": 0.30}]}
{"task": "ChartQA", "accuracy": 0.196, "name": "MMInstruct 75K from 150K Subset (SAS)", "sub_tasks": [{"name": "relaxed_augmented_split", "accuracy": 0.3392}, {"name": "relaxed_human_split", "accuracy": 0.2736}]}
{"task": "ChartQA", "accuracy": 0.3124, "name": "MMInstruct 75K from 150K Subset (Fac Loc)", "sub_tasks": [{"name": "relaxed_augmented_split", "accuracy": 0.3352}, {"name": "relaxed_human_split", "accuracy": 0.2896}]}
{"task": "ChartQA", "accuracy": 0.2024, "name": "MMInstruct 75K from 150K Subset (Low Loss)", "sub_tasks": [{"name": "relaxed_augmented_split", "accuracy": 0.1816}, {"name": "relaxed_human_split", "accuracy": 0.2232}]}
{"task": "ChartQA", "accuracy": 0.3292, "name": "MMInstruct 75K from 150K Subset (High Loss)", "sub_tasks": [{"name": "relaxed_augmented_split", "accuracy": 0.3696}, {"name": "relaxed_human_split", "accuracy": 0.2888}]}
