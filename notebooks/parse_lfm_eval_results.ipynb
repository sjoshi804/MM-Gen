{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import os\n",
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "\n",
    "def parse_metric_results(filename, name):\n",
    "    num_correct = {}\n",
    "    num_total = {}\n",
    "    total = 1\n",
    "    total_correct = -1\n",
    "    correct_per_q = []\n",
    "    with jsonlines.open(filename, \"r\") as reader:\n",
    "        pbar = tqdm(enumerate(reader), desc=f\"Parsing {filename}\")\n",
    "        for i, item in pbar:\n",
    "            subtask = item[\"category\"]\n",
    "            if subtask not in num_total:\n",
    "                num_total[subtask] = 0\n",
    "                num_correct[subtask] = 0\n",
    "            \n",
    "            num_total[subtask] += 1\n",
    "            \n",
    "            if item[\"MMMUMetric_result\"] == \"correct\":\n",
    "                num_correct[subtask] += 1\n",
    "                correct_per_q.append([f\"mmmu__{i}\", True])\n",
    "            else:\n",
    "                correct_per_q.append([f\"mmmu__{i}\", False])\n",
    "            \n",
    "            total_correct = sum([num_correct[key] for key in num_correct.keys()])\n",
    "            total = sum([num_total[key] for key in num_total.keys()])\n",
    "            \n",
    "            pbar.set_postfix_str(f\"overall_acc: {total_correct / total}\")\n",
    "            \n",
    "        pd.DataFrame(correct_per_q).to_csv(os.path.join(\"/home/t-sijoshi/skill-set-mazda\", f\"{name}.csv\"))   \n",
    "        summary_entry = {\n",
    "        \"task\": \"mmmu\",\n",
    "        \"name\": name,\n",
    "        \"metric_results_path\": filename,\n",
    "        \"accuracy\": total_correct / total,\n",
    "        \"num_correct\": total_correct,\n",
    "        \"num_total\": total,\n",
    "        \"sub_tasks\": []\n",
    "        }\n",
    "        \n",
    "        for sub_task in num_total.keys():\n",
    "            summary_entry[\"sub_tasks\"].append(\n",
    "                {\n",
    "                    \"name\": sub_task,\n",
    "                    \"frac_of_data\": num_total[sub_task] / total,\n",
    "                    \"num_correct\": num_correct[sub_task],\n",
    "                    \"num_total\": num_total[sub_task],\n",
    "                    \"accuracy\": num_correct[sub_task] / num_total[sub_task]\n",
    "                }\n",
    "            )\n",
    "    return summary_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing /home/t-sijoshi/LFM-Eval-Understand/logs/MMMU_PIPELINE/08-21-science-improve/v1_format/eval_report/metric_results.jsonl: 900it [00:00, 2204.33it/s, overall_acc: 0.25]               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task': 'mmmu', 'name': 'v1_format', 'metric_results_path': '/home/t-sijoshi/LFM-Eval-Understand/logs/MMMU_PIPELINE/08-21-science-improve/v1_format/eval_report/metric_results.jsonl', 'accuracy': 0.25, 'num_correct': 225, 'num_total': 900, 'sub_tasks': [{'name': 'Art and Design', 'frac_of_data': 0.13333333333333333, 'num_correct': 35, 'num_total': 120, 'accuracy': 0.2916666666666667}, {'name': 'Business', 'frac_of_data': 0.16666666666666666, 'num_correct': 44, 'num_total': 150, 'accuracy': 0.29333333333333333}, {'name': 'Science', 'frac_of_data': 0.16666666666666666, 'num_correct': 34, 'num_total': 150, 'accuracy': 0.22666666666666666}, {'name': 'Health and Medicine', 'frac_of_data': 0.16666666666666666, 'num_correct': 37, 'num_total': 150, 'accuracy': 0.24666666666666667}, {'name': 'Humanities and Social Science', 'frac_of_data': 0.13333333333333333, 'num_correct': 23, 'num_total': 120, 'accuracy': 0.19166666666666668}, {'name': 'Tech and Engineering', 'frac_of_data': 0.23333333333333334, 'num_correct': 52, 'num_total': 210, 'accuracy': 0.24761904761904763}]}\n"
     ]
    }
   ],
   "source": [
    "base_path = \"/home/t-sijoshi/LFM-Eval-Understand/logs/MMMU_PIPELINE/08-21-science-improve\"\n",
    "baselines = [\"v1_format\"]\n",
    "\n",
    "\n",
    "for method in baselines:\n",
    "    summary_entry = parse_metric_results(os.path.join(base_path, method, \"eval_report\", \"metric_results.jsonl\"), method)\n",
    "    print(summary_entry)\n",
    "    with jsonlines.open(\"/home/t-sijoshi/multimodal-data-gen/results/summary.jsonl\", mode='a') as writer:\n",
    "        writer.write(summary_entry)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
